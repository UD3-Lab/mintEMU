---
title: "The Legacy of the European Post-Master in Urbanism at TU Delft: A Text Mining Approach"
author:
  - Claudiu Forgaci:
      correspondence: "yes"
      email: C.Forgaci@tudelft.nl
      orcid: 0000-0003-3218-5102
      institute: tud
institute:
  - tud:
      name: Delft University of Technology
      address: Julianalaan 134, 2628 BL, Delft, Zuid-Holland, The Netherlands
title-block-published: "Last updated"  
date: now
date-format: long
format: pdf
  # docx:
  #   reference-doc: "../templates/template.docx" # Insert path for the DOCX file
  # pdf:
  #   template: "../templates/plos_latex_template.tex"
execute:
  echo: false
  warning: false
  message: false
  comment: "#>"
  fig-path: "../figures/"
  fig-dpi: 600
filters:
  - ../templates/scholarly-metadata.lua
  - ../templates/author-info-blocks.lua
  - ../templates/pagebreak.lua
bibliography: references.bib
csl: "../templates/journal-of-archaeological-science.csl" # Insert path for the bib-style
abstract: |
  Prompted by the closure of the European post-Master of Urbanism (EMU) at TU Delft,
  this paper aims to describe the legacy of the EMU. To that end, it reports on the
  analysis of the graduation theses produced over 2007-2021 when the program was running.
  The text mining approach allows for a close scrutiny of the latent thematic patterns, 
  and evolutions thereof, found in the full texts of the theses. Results reveal # topics
  that are to a large extent consistent with the set-up of the program in two thematic 
  semesters at TU Delft and three possible exchange semester locations, each characterised 
  by a specific approach. The paper discusses the relevance of the findings for the
  state of the profession and education in the field. Set up as a reproducible research
  compendium accompanied by a published FAIR dataset of the theses and an interactive
  dashboard, the analysis presented can be both reproduced and applied to other datasets. 
keywords: |
  urbanism; education; post-master; text mining; topic modeling
highlights: |
  These are the highlights. 
---

<!-- With the following code you can access and display values from the yml header above. -->

Keywords: `r rmarkdown::metadata$keywords`

Highlights: `r rmarkdown::metadata$highlights`

<!-- The actual document text starts here: -->

```{r setup}
#| include: false

# Load packages ----
## List packages to be loaded
packages <- c(
  "here",                                       # Managing paths
  # TODO remove reshape2 if `melt()` is replaced by`pivot_longer()`
  "tidyverse", "reshape2", "widyr", "readxl",   # Data manipulation and visualisation
  "furrr",                                      # Parallel use of purrr::map() functions
  "tidytext", "SnowballC", "tm",                # Text processing and analysis
  "topicmodels", "LDAvis", "tsne",              # Topic modeling
  "tidygeocoder",                               # Working with spatial data
  "igraph", "ggraph", "DiagrammeR"              # Network analysis and visualization
)

## Load required packages, install them if needed
for (package in packages) {
  if (!require(package, character.only = TRUE)) {
    install.packages(package,  "http://cran.us.r-project.org")
  }
  library(package, character.only = TRUE)
}

# Load analysis functions ----
devtools::load_all(".")
```

```{r load-data}
#| include: false

# Load thesis metadata
data(emu_metadata)

# Load raw text data
data(emu_raw) 

# Join text with the metadata file  
emu <- left_join(emu_raw, emu_metadata, by = "ID")
```

```{r clean-text}
#| include: false

emu$text_clean <- emu$text_raw |>
  clean_basic() 
  
```

# Introduction

The research presented in this paper was prompted by the closure of the European post-Master of Urbanism (EMU) of the Department of Urbanism at the Faculty of Architecture and the Built Environment, TU Delft. The EMU was an advanced master ran jointly by TU Delft, KU Leuven, UPC Barcelona and Università IUAV di Venezia. <!-- Complete paragraph about the program -->

Distinctive features of the program:\
- as a postmaster, it only accepted applicants with a prior MSc degree and experience in practice - provides a bridge towards PhD-level research - all students had an exchange semester at one of the other three program universities

In order to describe the legacy of the EMU program, including the distinctive features of its didactic approach, this paper aims to reveal the main topics taught in it and how those topics had evolved over the years of the program. To that end, we employed a text mining approach in which we analysed its output: 96 theses with an average of `r mean(str_length(emu$text_clean), na.rm = TRUE)` words produced over the years for the duration of the program between `r paste0(min(emu$graduation_year, na.rm = TRUE), "-", max(emu$graduation_year, na.rm = TRUE))`.

<!-- If the focus of the paper shifts to topic modeling the paragraphs below can be swapped with the paragraphs describing the EMU program. -->

This paper leverages topic modeling as an unsupervised machine learning method to reveal latent semantic structure and relationships between the latent topics in a large document collection [@muchene2021].

In Natural Language Processing (NLP) ... and the focus on its subtopic topic modeling

Topic modeling provides a way to generative

-   bag of words

-   decomposition into two low-dimensional matrices for doc-topic distribution and word-topic distribution, respectively

-   LDA and extensions of it (CTM, HDP, GLDA, DTM) are broadly adopted in topic modeling [@muchene2021]

<!-- Describe the research gaps -->

Research gaps and opportunities:

-   The scarcity of studies that use statistical methods for unstructured data in urbanism education

-   Theses are representative outputs of educational programs that can be used as data for text mining. The growing availability of theses in digital repositories presents an opportunity to that end. The TU Delft repository was established in 2007, in the year when the EMU program started.

-   Applications of topic modeling in the field of urbanism

-   Patterns revealed in past education can potentially better inform future decisions in education in the field

-   The EMU program...

Challenges:

Contribution:

-   This paper showcases reproducible research workflow. It is part of a reproducible research compendium and accompanied by a published dataset. The analyses presented in this paper were carried out in the free and open-source statistical programming software R [@rcoreteam2023]. The research compendium is available at \[ADD CITATION\], the data is published in th 4TU.ResearchData repository \[ADD CITATION\] and it is available for exploration in a Shiny dashboard at \[ADD LINK\]

-   Give an overview of th predominant topics and their evolution over the years of the EMU program. In addition to the natural document classification <!--# explain --> of documents provided by a repository, LDA allows documents to be part of multiple topics [@muchene2021].

<!--# Outline -->

The following section describes the case study data, pre-processing steps and the topic modeling workflow.

# Methods

Research questions:

-   What were the main topics addressed in the EMU program?

-   How did those topics evolve throughout the duration of the program between 2007-2021?

-   To what extent were the topics and evolution thereof influenced by the assignments given to the students throughout their studies leading up to their thesis?

-   To what extent were the topics and evolution thereof influenced by the exchange semester followed by the students?

    -   Use LDA clustering with 3 classes (corresponding to the three exchange universities) to see how well the clustering is aligned with where the student carried out their exchange semester. Use a confusion matrix to visualize the relation between real values and values assigned through clustering.
  
The research process (@fig-process-diagram)...

```{r fig-process-diagram}
#| fig-cap: Text mining process

mermaid("
graph LR
    A[Data collection]-->B[Pre-processing]
    B-->C[Information extraction]
    C-->D[Visualisation]
    C-->B
    D-->E[Results]
")
```

## Data collection

Of the total of 96 theses published in the EMU program, full-text documents were available and permissions were granted for `r nrow(emu)` <!--# update after removing theses without permission -->. Metadata was downloaded from the TU Delft Education Repository or obtained from the EMU program coordination records when unavailable on the repository. The full-text documents were obtained either from the TU Delft Education Repository (an API for downloading a large set of full-text data is not available), when available, or from the authors via email. Even though repository entries are open access, it was unclear if the content of the theses can be mined. Considering the relatively small size of the corpus and the possibility of directly contacting the alumni network, permissions were requested individually. Only PDFs for which permission was obtained were included in the corpus.With the exception of a bilingual English-Spanish thesis, all thesis were written in English. The theses are available in PDF format with complex layouts typical to an urbanism project in which text and various types of graphics are combined. @tbl-data gives an overview of the data included in the analysis.

Description of text extraction; raw data available on 4TU.ResearchData at...

```{r tbl-data}
#| tbl-cap: Overview of theses included in the analysis
#| tbl-cap-location: top

```

The imported dataset contains the text data in the `text` column and related metadata in the other columns...

For each thesis, henceforth referred to as "document", the metadata consisted of the `ID`, `title`, `graduation_year`, `graduation_semester`, `exchange_semester`, `location`, `latitude`, `longitude`, `abstract`, `text_raw` and `text_clean` <!--# check column names or add them in an inline code chunk with epoxy -->.

Both the raw and cleaned data are published in the 4TU.ResearchData repository [ADD CITATION].

```{r fig-map-theses}
#| fig-cap: "Location of EMU theses included in the analysis"

# Show thesis locations on world map
visualize_thesis_locations(emu)
```

@fig-map-theses shows the geographic spread of the theses included in the analysis.

## Pre-processing

Word-level tokenisation was applied with the `unnest_tokens()` function of the `tidytext` package. This was preferred over other tokenisers as it produced results that work seamlessly with other `tidyverse` tools used in the analysis. `unnest_tokens()` removes punctuation, converts to lower-case, and separate all words into individual tokens.

Bigrams such as "public space" and trigrams such as "socio-spatial segregation" identified during preliminarey EDA were concatenated by replacing white spaces with underscores.

For analyses in which word-level tokens were used, stop words, i.e., common words or words that do not add much meaning, were removed in the pre-processing stage. Global, subject-specific and document stop words [@hvitfeldt2021a] were differentiated. As all theses were written in English, the pre-made "snowball" lexicon was used to remove global stop words. <!-- We tried to create a custom global stop word list on the entire corpus, by removing high-frequency words, starting with 20 words and increasing by 10 until reaching words that are not appropriate as stop words, but no significant improvement was observed in the result. --> We manually constructed a list of subject-specific stop words based on our domain knowledge. Words that are commonly used to structure a theses, such as "preface", "contents", and "introduction", were added as subject-specific stop words as well. Finally, titles and author names were removed as document stop words. In the next pre-processing step, we normalised the resulting words, that is, reduced them to their canonical dictionary forms, the resulting words were lemmatised [@hvitfeldt2021a]. Lemmatisation was preferred over stemming as the former tends to be more meaningful and less degrading than the latter.

```{r stop-words}

# TODO replace with cleaning steps including ngram concatenation
# Add cleaned text column with removed stop words
emu$text_clean <- emu$text_clean |>
  str_remove_all(find_meta_stopwords(emu)) |> 
  str_remove_all(urbanism_stopwords(
    add_stopwords = c("emu", "european postgraduate master in urbanism", "tu delft", "ku leuven", "upcs barcelona", "iuav venice", 
                      "space", "spatial", "spaces", "plan", "public", "development"))) |>
  str_remove_all(thesis_stopwords(add_stopwords = c("advisor", "prof", "fig", "ure", "ning")))

# Unnest words and remove words of one and two letters
emu_words <- emu |> 
  select(title, text_clean) |> 
  unnest_tokens(output = word, input = text_clean) |>  # remove punctuation, convert to lower-case, separate all words
  anti_join(tidytext::stop_words, by = "word") |>  # remove stop words
  mutate(word = textstem::lemmatize_words(word))  # lemmatise words
```

Finally, additional checks were made to the document in search for previously undetected stop words. Short words of one and two letters could be safely removed, while three-letter words were kept, as many of those, including "map" and "low", were meaningful and occurring in high frequency.

```{r short-stop-words}
#| include: false

# short_words(emu_words$word, l_word = 3, l_list = 100)

emu_words <- emu_words |>  
  filter(nchar(word) >= 3)  # keep only words that are at least three letters long

```

## Information extraction

### Topic model

Similar to clustering on numeric data, topic modelling is a method of unsupervised classification of topics found in a group of documents [@silge2017]. We used the Latent Dirichlet Allocation (LDA) algorithm for topic modeling. In LDA, each document is a mixture of topics and each topic is a mixture of words. The probability of a term being part of a topic is given by the $\beta$ (beta) statistic, while the probability of a document being part of a topic is given by the $\theta$ (theta) statistic.

LDA is a Bayesian generative topic model which assumes that each topic is a distribution over words and each document is a distribution over topics [@blei2012; @grün2011; @blei2003].

Latent topics are measured as patterns of word co-occurrence

Describe model parameters:

-   collection of M = `r nrow(emu)` documents

-   corpus D

-   N = `r length(emu_words$word)` words

-   vocabulary of V = `r length(unique(emu_words$word))` words

Determining tuning parameters:

-   The number of topics was qualitatively determined

LDA requires a document-term matrix as input.

```{r topic-model-lda-1}

# Prepare document-term matrix as input for LDA model
# Reduce the size of the DTM by excluding words with a frequency lower than 5
emu_dtm <- convert_to_dtm(emu_words, 
                          title_col = "title", 
                          word_col = "word", 
                          min_term_freq = 5)

# Fit LDA model with K clusters
## Determine K value empirically
k = get_number_of_topics(emu_dtm)
# TODO run the model with the empirically determined k and compare (or combine) with results using qualitative value
emu_lda_k <- LDA(emu_dtm, k = k, method = "Gibbs",
               control = list(seed = 2023, iter = 500, verbose = 25))


## Determine K value qualitatively
## K = 5 corresponds to the 2 TUD semesters and the 3 exchange semesters combined 
emu_lda_5 <- LDA(emu_dtm, k = 5, method="Gibbs",
               control = list(seed = 2023, iter = 500, verbose = 25))

# Extract beta and theta statistics from LDA model
beta_5 <- posterior(emu_lda_5)$terms
theta_5 <- posterior(emu_lda_5)$topics 

beta_k <- posterior(emu_lda_k)$terms
theta_k <- posterior(emu_lda_k)$topics 

# Add pseudo-names to topics based on the top 5 words in each topic
topic_names_k <- name_topics(beta_k)
topic_names_5 <- name_topics(beta_5)

```

```{r fig-topics-per-doc}
#| fig-cap: Topic probability distribution per document

# TODO Replace this with a table containing topics, documents, and theta
#      sorted by theta in decreasing order
# What is the dominant topic in each document?
N = 70
topic_prop_examples <- theta_5[1:N,]
colnames(topic_prop_examples) <- topic_names_5
# TODO change to pivot_longer
vizDataFrame <- melt(cbind(data.frame(topic_prop_examples), 
                           document = factor(str_sub(rownames(topic_prop_examples), 1, 20))), 
                     variable.name = "topic", id.vars = "document") |> 
  group_by(document) |> 
  mutate(value_max = max(value),
         topic_max = topic[value == value_max])
ggplot(data = vizDataFrame, aes(topic, value, fill = topic_max), ylab = "proportion") + 
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  coord_flip() +
  facet_wrap(~ document, ncol = 10) +
  theme(legend.position = "bottom") +
  scale_fill_viridis_d(name = "Topic") +
  labs(title = "Theses as distributions over topics",
       subtitle = "Charts colored by the highest probability")

```

```{r topics-per-corpus}

# What are the most probable topics in the entire collection?
topicProportions <- colSums(theta) / nDocs(emu_dtm)  # mean probabilities over all paragraphs
names(topicProportions) <- topic_names     # assign the topic names we created before
sort(topicProportions, decreasing = TRUE) # show summed proportions in decreased order

soP <- sort(topicProportions, decreasing = TRUE)
paste(round(soP, 5), ":", names(soP))

# How many documents are with the highest probability in each topic?
K = 5
countsOfPrimaryTopics <- rep(0, K)
names(countsOfPrimaryTopics) <- topic_names
for (i in 1:nDocs(emu_dtm)) {
  topicsPerDoc <- theta[i, ] # select topic distribution for document i
  # get first element position from ordered list
  primaryTopic <- order(topicsPerDoc, decreasing = TRUE)[1] 
  countsOfPrimaryTopics[primaryTopic] <- countsOfPrimaryTopics[primaryTopic] + 1
}
sort(countsOfPrimaryTopics, decreasing = TRUE)

so <- sort(countsOfPrimaryTopics, decreasing = TRUE)
paste(so, ":", names(so))
```

```{r}

topicToFilter <- 6  # you can set this manually ...
# ... or have it selected by a term in the topic name (e.g. 'children')
topicToFilter <- grep('river', topic_names)[1] 
topicThreshold <- 0.2
selectedDocumentIndexes <- which(theta[, topicToFilter] >= topicThreshold)
filteredCorpus <- emu_theses[selectedDocumentIndexes,]
# show length of filtered corpus
filteredCorpus
```

```{r}

N = 82
topic_prop_per_year <- theta[1:N,]
colnames(topic_prop_per_year) <- topic_names

# TODO change to pivot_longer
vizDataFrameWithYear <- melt(cbind(data.frame(topic_prop_per_year), 
                           document = factor(rownames(topic_prop_per_year))), 
                     variable.name = "topic", id.vars = "document") |> 
    left_join(select(emu_theses, title, "year" = graduation_year), by = c("document" = "title"))

# Plot topic proportions per year
vizDataFrameWithYear |> 
  ggplot(aes(x = factor(year), y = value, fill = topic)) + 
  geom_bar(stat = "identity", position = "fill") +
  scale_fill_viridis_d(name = "Topics") +
  xlab("Graduation year") +
  ylab("Proportion of topics") +
  labs(title = "Topic proportions per year")
```

```{r lda-vis}

# TODO check if this can be included in the Shiny dashboard
# Function to approximate the distance between topics
svd_tsne <- function(x) tsne(svd(x)$u)

# Convert DTM into JSON required by the LDAvis package
json <- LDAvis::createJSON(
  phi = beta, 
  theta = theta, 
  doc.length = rowSums(as.matrix(emu_dtm)), 
  vocab = colnames(emu_dtm), 
  term.frequency = colSums(as.matrix(emu_dtm)),
  mds.method = svd_tsne,
  plot.opts = list(xlab="", ylab="")
)

# Visualise topics model with LDAvis
LDAvis::serVis(json)

```

```{r topic-model-beta}

# TODO Explore including concatenated collocations in the topic model

# Extract topics as mixtures of terms from LDA model
emu_topics <- tidy(emu_lda, matrix = "beta")

# The top n most common words in each topic
emu_top_terms <- get_top_words_per_topic(emu_topics, 20)

# TODO create visualisation function
emu_top_terms |> 
  ggplot(aes(reorder(term, beta), beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

# TODO can this be generalised to more than two topics?

beta_spread <- emu_topics |> 
  mutate(topic = paste0("topic", topic)) |> 
  spread(topic, beta) |> 
  filter(topic1 > 0.001, topic2 > .001) |> 
  mutate(log_ratio = log2(topic2/topic1))

beta_spread %>%
  group_by(direction = log_ratio > 0) %>%
  slice_max(abs(log_ratio), n = 20) %>% 
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(log_ratio, term)) +
  geom_col() +
  labs(x = "Log2 ratio of beta in topic 2 / topic 1", y = NULL)
```

```{r topic-model-gamma}

# Extract documents as mixtures of topics from LDA model
emu_documents <- tidy(emu_lda, matrix = "gamma")

emu_documents |> 
  ggplot(aes(x = reorder(str_sub(document, 1, 50), gamma), y = gamma, fill = topic)) + 
  geom_bar(stat = "identity") + 
  coord_flip()

emu_documents |> 
  inner_join(emu |> 
               select(title, graduation_year), 
             by = c("document" = "title")) |> 
  ggplot(aes(x = reorder(str_sub(document, 1, 50), gamma), y = gamma, fill = topic)) + 
  geom_bar(stat = "identity") + 
  coord_flip() + 
  facet_wrap(~ graduation_year, ncol = 1, scales = "free", drop = TRUE)

emu_documents |> 
  inner_join(emu |> 
               select(title, graduation_year), 
             by = c("document" = "title")) |> 
  ggplot(aes(x = factor(graduation_year), y = topic, fill = factor(topic))) +
  geom_col()
```

# Results

## Overall patterns

The tf-idf statistic shows that...

## Topics

## The dynamics of thesis topics

Topics such as ... have gained importance over the years, while ... have lost prominence.

```{r top-words-document}

# Which are the top n most frequently used words in each thesis?
get_top_words_per_document(data = emu_words, top_n = 10)
```

```{r top-words-corpus}

# Which are the top n most frequently used words in all theses?
# TODO add `topic` to `fill` aesthetic
n = 50
get_top_words_per_corpus(data = emu_words, top_n = n) |>
  ggplot(aes(reorder(word, n), n)) +
  geom_col(fill = "orange") +
  geom_text(aes(label = n), size = 2, hjust = 1.1) +
  coord_flip() +
  xlab("Word") +
  ylab("Frequency") +
  labs(title = paste0("Top ", n, " most used words in the corpus of theses")) +
  theme_minimal() +
  theme(panel.grid = element_blank())
```

```{r}
#| label: ngrams
#| dependson: clean-text

## Bigrams
emu_bigrams <- emu |> 
  get_ngrams(n = 2, title_col = "title", text_col = "text_clean", stem = TRUE)

bigram_counts <- emu_bigrams |> 
  count(w_1, w_2, sort = TRUE)

bigram_stem_counts <- emu_bigrams |> 
  count(w_1_stem, w_2_stem, sort = TRUE)

bigram_counts
bigram_stem_counts

## Trigrams
emu_trigrams <- emu |> 
  get_ngrams(n = 3, title_col = "title", text_col = "text_raw")

trigram_counts <- emu_trigrams |> 
  count(w_1, w_2, w_3, sort = TRUE)

trigram_counts

## Bigram tf-idf
bigram_tf_idf <- emu_bigrams |> 
  count(title, ngram) |> 
  bind_tf_idf(ngram, title, n) |> 
  arrange(desc(tf_idf))

bigram_tf_idf

## Trigram tf-idf
trigram_tf_idf <- emu_trigrams |> 
  count(title, ngram) |> 
  bind_tf_idf(ngram, title, n) |> 
  arrange(desc(tf_idf))

trigram_tf_idf
```

```{r}
## Show network of bigrams
bigram_graph <- bigram_counts |> 
  filter(n > 100) |> 
  graph_from_data_frame()

bigram_graph

set.seed(2023)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
  geom_node_point(color = "lightblue", linewidth = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

## Higgh frequency bigrams ----

emu_bigrams_top <- emu_bigrams |>
  group_by(title) |>
  count(ngram, sort = TRUE) |>
  slice_max(n, n = 20) |>        ### top_n is superseded; better use slice_max instead
  # filter(n > 50)                ### filter words with a minimum count
  ungroup() |>
  mutate(bigram = reorder(ngram, n))

emu_bigrams_top_all <- emu_bigrams |>
  count(ngram, sort = TRUE) |>
  slice_max(n, n = 20) |>        ### top_n is superseded; better use slice_max instead
  # filter(n > 50)                ### filter words with a minimum count
  mutate(ngram = reorder(ngram, n))

emu_bigrams_top |>
  filter(title == unique(title)[1:10]) |> 
  ggplot(aes(x = ngram, y = n, fill = title)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(y = "Count",
       x = "Unique words",
       title = "Count of unique words found in thesis",
       subtitle = "Stop words were removed from the list") +
  facet_wrap( ~ title, scales = "free", ncol = 2) +
  theme(legend.position = "none") 
```

The `tf-idf` statistic shows words that are the most important to one document in a collection of documents [@silge2017]. In this case, it shows high values for the names of thesis locations that tend to be high-frequency thesis-specific words.

```{r tf-idf}

## Calculate tf-idf statistic
emu_tf_idf <- emu_words |> 
  get_tf_idf(title_col = "title", word_col = "word")

## Get top n words most specific to each thesis 
emu_tf_idf |> 
  group_by(title) |> 
  top_n(5, tf_idf) |> 
  arrange(desc(tf_idf), .by_group = TRUE)

```

```{r}

# Preview term frequency distributions in a subset of theses
emu_words_count <- emu_words |> 
  count(title, word, sort = TRUE) |> 
  ungroup()

emu_words_total <- emu_words_count |> 
  group_by(title) |> 
  summarise(total = sum(n))

emu_words_count <- left_join(emu_words_count, emu_words_total)

ggplot(emu_words_count |> 
         filter(title == unique(title)[1:6]),
       aes(n/total, fill = title)) +
  geom_histogram(show.legend = FALSE, binwidth = 0.0001) +
  xlim(NA, 0.0009) +
  facet_wrap(~ title, ncol = 3, scales = "free_y")
```

```{r}
# Examine Zipf's law for the EMU theses
## Calculate rank and term frequency
freq_by_rank <- emu_words_count |> 
  group_by(title) |> 
  mutate(rank = row_number(),
         `term frequency` = n/total)

## Visualise Zipf's law
freq_by_rank |> 
  ggplot(aes(rank, `term frequency`, color = title)) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()

rank_subset <- freq_by_rank |> 
  filter(rank < 500,
         rank > 10)

lm(log10(`term frequency`) ~ log10(rank), data =  rank_subset)

freq_by_rank |> 
  ggplot(aes(rank, `term frequency`, color = title)) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
  geom_abline(intercept = -1.44, slope = -0.7, color = "gray50", linetype = 2) +
  scale_x_log10() +
  scale_y_log10()
```


```{r}
#| eval: false

## Visualise the top n words with the highest tf-idf values for each of a subset of theses
emu_words_count |> 
  get_tf_idf(title_col = "title", word_col = "word") |> 
  filter(title == unique(title)[1:2]) |>  # Keep only the first 6 theses
  arrange(desc(tf_idf)) |> 
  mutate(word = factor(word, levels = rev(unique(word)))) |> 
  group_by(title) |> 
  top_n(15) |> 
  ungroup() |> 
  ggplot(aes(word, tf_idf, fill = title)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~ title,  ncol = 3, scales = "free") +
  coord_flip()
```

### Word embeddings

\[Describe word embeddings ...\]

```{r word-embeddings-pmi}

# To what extent do words co-occur with other words?
tidy_pmi <- get_pmi(emu, title_col = "title", text_col = "text_clean")
```

The high-dimensional sparse matrix of word features is projected into reduced, 100-dimensional set of features.

```{r word-embeddings-svd}

# Determine word vectors using SVD, a method for dimensionality reduction
tidy_word_vectors <- tidy_pmi |> 
  widely_svd(
    item1, item2, pmi,
    nv = 100, maxit = 1000
  )
```

\[What terms should be checked with the function below?\]

```{r word-embeddings-nn}

# Calculate nearest neighbours of a given word
tidy_word_vectors |> 
  nearest_neighbors("resilience")
```

The first \[?\] components... \[what do they show?\]

```{r word-embeddings-vis}

# The first n components with top words found in the corpus of theses
visualise_first_n_components(tidy_word_vectors, 24, 12)
```

```{r doc-embeddings}

tidy_words <- emu |> 
  select(title, text_clean) |> 
  unnest_tokens(word, text_clean) |> 
  add_count(word) |> 
  filter(n >= 50) |> 
  select(-n)
  
# Summarise word embeddings into document embeddings for modeling purposes
word_matrix <- tidy_words |> 
  count(title, word) |> 
  cast_sparse(title, word, n)

embedding_matrix <- tidy_word_vectors |> 
  cast_sparse(item1, dimension, value)

doc_matrix <- word_matrix %*% embedding_matrix

dim(doc_matrix)
```

# Discussion

-   Although the focus of this study is on describing a fixed corpus though unsupervised machine learning, it can provide input into supervised predictive models.

# Conclusion

# Acknowledgements

<!-- The following line inserts a page break  -->

\newpage

# References

<!-- The following line ensures the references appear here for the MS Word or HTML output files, rather than right at the end of the document (this will not work for PDF files):  -->

::: {#refs}
:::

\newpage

### Colophon

This report was generated on `r Sys.time()` using the following computational environment and dependencies:

```{r}
#| label: colophon
#| cache: false

# which R packages and versions?
if ("devtools" %in% installed.packages()) devtools::session_info()
```

The current Git commit details are:

```{r}
# what commit is this file at? 
if ("git2r" %in% installed.packages() & git2r::in_repository(path = ".")) git2r::repository(here::here())  
```
