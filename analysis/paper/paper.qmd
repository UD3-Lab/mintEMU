---
title: "The Legacy of the European Post-Master in Urbanism at TU Delft: A Text Mining Approach"
author:
  - name: Claudiu Forgaci
    correspondence: "yes"
    email: C.Forgaci@tudelft.nl
    orcid: 0000-0003-3218-5102
    affiliations: 
      - ref: tud
affiliations:
  - id: tud
    name: Delft University of Technology
    address: Julianalaan 134, 2628 BL, Delft, Zuid-Holland, The Netherlands
title-block-published: "Last updated"  
date: now
date-format: long
format:
  html: default
  # docx:
  #   reference-doc: ../templates/template.docx
execute:
  echo: false
  warning: false
  message: false
  comment: "#>"
  fig-path: "../figures/"
  fig-dpi: 600
filters:
  - ../templates/scholarly-metadata.lua
  - ../templates/author-info-blocks.lua
  - ../templates/pagebreak.lua
bibliography: references.bib
csl: "../templates/journal-of-archaeological-science.csl" # Insert path for the bib-style
abstract: |
  This paper aims to describe the legacy of the European post-Master of Urbanism 
  (EMU) that ran between 2005 and 2021 at TU Delft. To that end, it reports on the
  analysis of the graduation theses produced over the duration of the program.
  The text mining approach allows for a close scrutiny of the latent thematic patterns, 
  and evolutions thereof, found in the full texts of the theses. Results reveal 5 topics
  that are to a large extent consistent with the set-up of the program in two thematic 
  semesters at TU Delft and three possible exchange semester locations, each characterised 
  by a specific approach. The paper discusses the relevance of the findings for the
  state of the profession and education in the field. Set up as a research
  compendium accompanied by a published FAIR dataset and an interactive dashboard, 
  the research is fully reproducible and enables further analysis. 
keywords: |
  urbanism; education; post-master; text mining; topic modeling
highlights: |
  These are the highlights. 
---

<!-- The following code display values from the YAML header above. -->

Keywords: `r rmarkdown::metadata$keywords`

Highlights: `r rmarkdown::metadata$highlights`

<!-- Setup code, not shown in the rendered document. -->

```{r setup}
#| include: false

# TODO remove unused packages after all analysis has been translated into functions
# Load packages ----
## List packages to be loaded
packages <- c(
  "here",
  "tidyverse", "widyr", "readxl",   # Data manipulation and visualisation
  "furrr",                          # Parallel use of purrr::map() functions
  "tidytext", "SnowballC", "tm",    # Text processing and analysis
  "topicmodels", "LDAvis", "tsne",  # Topic modeling
  "tidygeocoder",                   # Working with spatial data
  "igraph", "ggraph", "DiagrammeR"  # Network analysis and visualization
)

## Load required packages, install them if needed
for (package in packages) {
  if (!require(package, character.only = TRUE)) {
    install.packages(package,  "http://cran.us.r-project.org")
  }
  library(package, character.only = TRUE)
}


# TODO should this be replaced with a call to `library(mintEMU)`?
# Load analysis functions ----
devtools::load_all(".")
```

```{r load-data}
#| include: false

# TODO merge into setup code chunk

# Load data for analysis
data(emu_metadata)  # Thesis metadata
data(emu_raw)       # Raw text data
data(emu_clean)     # Cleaned text data

# Join metadata and cleaned text data
emu <- emu_metadata |> 
  left_join(emu_raw, by = "ID") |> 
  left_join(emu_clean, by = "ID")

theses_all_per_year <- read_csv(file = here("analysis", "data", "derived_data", "emu-theses-per-year.csv"))

word_counts <- get_word_counts(emu, text_col = "text_clean")

```

<!-- The actual document text starts here: -->

# Introduction {#intro}

The research presented in this paper was prompted by the closure of the European post-Master of Urbanism (EMU) at TU Delft. The EMU is an advanced program started jointly by TU Delft, KU Leuven, UPC Barcelona and Università IUAV di Venezia in 2005. The program distinguished itself internationally as a post-master open only to applicants with a prior MSc degree and practical experience in urbanism or a related field. With a combined research and design curriculum, it provided a bridge towards both advanced practice and research trajectories after graduation. All students followed an exchange semester at one of the other three program universities, allowing them to have a broad, international view on the field of urbanism. Although located in Europe and named "European", its geographic scope was global, visible especially at the stage of graduation, considering that a large majority of students were of non-European origin and chose to work on a case from their home location.

In 2021, TU Delft stepped out from the program, but its approach has remained imprinted on the TU Delft approach to urbanism. Its core topics Urban Region Networks (Semester 1) and Constructing Sustainable Urban Landscapes (Semester 2), as well as the influence of the partner universities, mostly IUAV Venice where most TU-Delft-hosted students spent their exchange semesters, are visible in the current MSc Urbanism and, to some extent, MSc Landscape Architecture tracks. There is general agreement on the considerable impact of the EMU on education and practice targeting sustainable urbanism, but why and how it is so remains implicit. Understanding what made the program impactful would be beneficial for targeted urbanism education and practice in response to current societal challenges. Having a close look at the assignments that were addressed by the program as well as research and design approaches used to tackle those topics throughout the years can provide valuable insight into what are the core, stable elements of urbanism practice, on the one hand, and how does the field need to adapt to changing societal challenges, on the other.

To describe the legacy of the EMU program, including the distinctive features of its didactic approach, this paper aims to reveal the main topics taught in it and how those topics had evolved over the years of the program. To that end, the research presented in this paper employed a text mining approach targeting the output of the EMU: 96 theses with an rounded average of `r round(mean(word_counts, na.rm = TRUE), -3) |> format()` words produced over the years for the duration of the program between `r paste0(min(emu$grad_year, na.rm = TRUE) - 2, "-", max(emu$grad_year, na.rm = TRUE))`. Although the corpus of thesis is relatively small, it is large enough to make a manually conducted quantitative analytical approach unfeasible.

## A text mining approach {#textmin}

**The rise of computational social sciences** [@wallach2018] provides tools and methods to summarise large sets of data that would not be possible with human annotation [@blei2012]. Often such data is available in unstructured textual form and formats that are not easily machine-readable, such as Adobe PDF or Microsoft Word documents [@benchimol2022]. While quantitative text analysis have been widely adopted in computationla social sciences such as linguistics, communication, and political science [@benchimol2022], urbanism research, in general, and urbanism education studies, in particular, rarely use statistical methods such as topic modeling to analyse unstructured data. This is especially a missed opportunity considering that theses are representative outputs of educational programs that are increasingly available in digital repositories - the TU Delft repository was established in 2007, which is also the first graduation year of the EMU, and has been steadily growing since then. Patterns revealed in past education can potentially better inform future decisions in education in the field.

Acknowledging that opportunity, **this paper leverages topic modeling**, a branch of Natural Language Processing, as an unsupervised machine learning method to reveal latent semantic structure and relationships between latent topics in a large document collection [@muchene2021]. Latent Dirichlet Allocation (LDA) is broadly adopted in topic modeling [@muchene2021] as it uses a soft clustering algorithm in which objects can be part of multiple clusters (mixed membership approach) as opposed to hard clustering algorithms (single membership approaches) [@rüdiger2022a]. LDA uses as input a bag-of-words (BOW) model, more generally called the vector space model (VSM), in which documents in a corpus are decomposed into two low-dimensional matrices for document-topic distribution and word-topic distribution, respectively. Although topic modeling in general automates the process of identifying topics through clustering, those topics need to be manually interpreted and labelled after the application of the algorithm [@rüdiger2022a]. Moreover, arguing for the right algorithm and a suitable number of clusters remains a challenge [@rüdiger2022].

In addition to describing the legacy of the EMU, **this paper showcases a reproducible research workflow**. Together with a published FAIR dataset, it forms a reproducible research compendium. The analyses presented in this paper were carried out in the free and open-source statistical programming software R [@rcoreteam2023]. The research compendium is available at \[ADD CITATION\], the data is published in th 4TU.ResearchData repository [@forgaci2023] and it is available for exploration in a Shiny dashboard at <http://mintemu_shiny.bk.tudelft.nl:3838/mintemu/>.

The following section describes the case study data, pre-processing steps and the topic modeling workflow. The results of the analysis are then summarised in response to two research questions. The paper concludes with a discussion on what the lessons learned from the EMU program mean for urbanism education and practice in general as well as on the broader challenges and opportunities of the text mining approach taken in the paper.

# Methods {#methods}

To describe the legacy of the EMU program this research answers two questions: What were the main topics addressed in the EMU program? And how did those topics evolve over the duration of the program? The paper then discusses to what extent those topics and evolution thereof were influenced by the assignments given to the students throughout their studies leading up to their thesis and by the exchange semester followed by the students. The research process is described in @fig-process-diagram.

```{r fig-process-diagram}
#| fig-cap: Text mining process
#| eval: false

mermaid("graph LR
    A[Data collection]-->B[Pre-processing]
    B-->C[Information extraction]
    C-->B
    C-->D[Visualisation]
    D-->E[Results]
    ")
```

## Data collection {#datacol}

**The dataset consists of full texts and metadata of a subset of EMU theses written in English.** The full texts are available as PDF documents with unstructured layouts typical to an urbanism project in which text and various types of graphics are combined <!--add figure-->. The full-text documents were obtained either from the TU Delft Education Repository upon request (an API for downloading full-text data was not available at the time of data collection), or from the authors via email if not available on the repository. Metadata was downloaded from the TU Delft Education Repository or obtained from the EMU program coordination records when unavailable on the repository. Even though repository entries are openly accessible, licenses were not specified and thus it was unclear if the content of the theses can be mined and if the resulting data can be published. Considering the relatively small size of the corpus and the possibility of directly contacting the alumni network, permissions were requested individually. As a result, of the total of `r sum(theses_all_per_year$n_total)` theses presented in the EMU program, full-text documents were obtained for `r sum(theses_all_per_year$n_pdfs)` of which permissions were granted for `r nrow(emu)`. Only PDFs for which permission was obtained were included in the corpus. The final corpus contains at least one thesis for each year of graduation between `r min(emu$grad_year)`-`r max(emu$grad_year)` as shown in @tbl-data-selection. @fig-map-theses shows the geographic spread of the theses included in the analysis.

```{r tbl-data-selection}
#| tbl-cap: Overview of theses included in the analysis
#| tbl-cap-location: top

theses_all_per_year |> 
  select(grad_year, n_total, n_permissions) |> 
  mutate(grad_year = as.factor(grad_year),
         `% of all theses` = scales::label_percent()(round(n_permissions / n_total, 4))) |>
  bind_rows(data.frame(grad_year = "", 
                       n_permissions = nrow(emu), 
                       n_total = sum(theses_all_per_year$n_total),
                       `% of all theses` = scales::label_percent()(nrow(emu) / sum(theses_all_per_year$n_total)),
                       check.names = FALSE)) |> 
  filter(!is.na(grad_year)) |> 
  rename(`No. theses` = n_total,
         `Graduation year` = grad_year,
         `No. theses included` = n_permissions) |>
  kableExtra::kable(align = "lrrr")

```

```{r fig-map-theses}
#| fig-cap: "Location of EMU theses included in the analysis"

# Show thesis locations on world map
visualize_thesis_locations(emu)
```

## Pre-processing {#prepro}

For each full text, henceforth referred to as "document", the following cleaning steps were performed:

1.  Text was extracted from the original PDF documents with the the PyMuPDF Python package. During extraction, the front matter (cover page, colophon, table of contents, acknowledgements) and back matter (references, acknowledgements, appendices) were excluded. The page numbers used to exclude the front matter and back matter were manually identified and used as input for page selection during text extraction. The extracted text was stored in the `text_raw` column.
2.  The extracted text was cleaned in R of line breaks, hyphenation, occurrences of the thesis title, author name, and page numbers. The cleaned text data was added to each thesis in a new column `text_clean`.
3.  From the metadata only the title, graduation year, exchange semester and location of the case studied in the theses were kept. Column names were shortened and updated to follow the naming conventions used in the dataset.

The resulting dataset consists of the id, `title`, `grad_year`, `exch_sem`, `loc`, `lat`, `lon`, `text_raw` and `text_clean`, as shown in @tbl-data-variables.

```{r tbl-data-variables}

# TODO make sure that the columns reported in the text match those included in the dataset

tibble::tibble(Column = names(emu), 
               Type = purrr::map(emu, class),
               Completeness = 
                 purrr::map(emu, is.na) |> 
                 purrr::map(mean) |> 
                 purrr::map(\(x){1 - as.numeric(x)}) |> 
                 purrr::map(scales::label_percent())) |>
  kableExtra::kable()

```

Both the raw and cleaned data are published in the 4TU.ResearchData repository [@forgaci2023].

## Information extraction {#infoext}

<!-- Add intro to this section -->

Information extraction is a process of extracting information from unstructured data following the steps of tokenisation, construction of a Document-Term Matrix (DTM), fitting the topic model

### Tokenisation

Word-level tokenisation was applied on the corpus. Bigrams such as "public space" and trigrams such as "socio-spatial segregation" identified with high frequency (at least 20 on a corpus level) during EDA were concatenated by replacing white spaces and hyphens with underscores. This way, they could be included alongside words in the topic model.

**Stop words, i.e., common words or words that do not add much meaning, were removed**. Global, subject-specific and document stop words [@hvitfeldt2021a] were differentiated. As all theses were written in English, the pre-made "snowball" lexicon was used to remove global stop words. We manually constructed a list of subject-specific stop words based on our domain knowledge. Words that are commonly used to structure a theses, such as "preface", "contents", and "introduction", were added as subject-specific stop words as well. Finally, titles and author names were removed as document stop words. In the next pre-processing step, we normalised the resulting words, that is, reduced them to their canonical dictionary forms, the resulting words were lemmatised [@hvitfeldt2021a]. Lemmatisation was preferred over stemming as the former tends to be more meaningful and less degrading than the latter.

```{r tokenize}
# Unnest tokens (words, bigrams and trigrams) and remove words of one and two letters
emu_words <- tokenize(emu)
```

Finally, additional checks were made to the document in search for previously undetected stop words. Short words of one and two letters could be safely removed, while three-letter words were kept, as many of those, including "map" and "low", were meaningful and occurring in high frequency.

### Document-term matrix

```{r dtm}
# Prepare document-term matrix as input for LDA model
emu_dtm <- convert_to_dtm(emu_words, 
                          id_col = "ID", 
                          word_col = "word", 
                          min_term_freq = 5)
```

The topic model requires a document-term matrix (DTM) as input. To reduce the size of the DTM, words with a frequency lower than 5 were excluded.

### Topic model

Similar to clustering on numeric data [@rüdiger2022], topic modelling is a method of unsupervised classification of topics found in a collection of documents [@silge2017]. We used the Latent Dirichlet Allocation (LDA) algorithm for topic modeling. LDA is a Bayesian generative topic model which assumes that each topic is a distribution over words and each document is a distribution over topics [@blei2012; @grün2011; @blei2003]. This means that each document is a mixture of topics and each topic is a mixture of words. The probability of a term being part of a topic is given by the $\beta$ (beta) statistic, while the probability of a document being part of a topic is given by the $\theta$ (theta) statistic.

The corpus (D) consists of a collection of M = `r nrow(emu)` documents with a total number of N = `r length(emu_words$word)` words and a vocabulary of V = `r length(unique(emu_words$word))` words. After the exclusion of words with a frequency lower than 5, the vocabulary was reduced to V = `r ncol(emu_dtm)`.

A key tuning parameter (i.e., a manually specified parameter) in an LDA model is the number of clusters K. A K with a smaller value will yield more general topics whereas a K with a high value will result in more specific topics. Hence, finding a suitable value entails a trade-off between interpretability and specificity. This is very much dependent on the data at hand and the research question. Given the small size of the corpus and the way the program was structured, the value of K = 5 was determined qualitatively considering the input received by the students in the two thematic semesters at TU Delft and the three alternative exchange semesters, each with a very distinct approach.

| University    | Semester        | Topic                                            |
|------------------|------------------|------------------------------------|
| TU Delft      | Fall semester   | Urban region networks                            |
|               | Spring semester | Constructing the sustainable city                |
| IUAV Venice   | Fall semester   | Territories of dispersion: Situations, Scenarios |
| KU Leuven     | Fall semester   |                                                  |
| UPC Barcelona | Fall semester   | Urban Transformations                            |

Each topic is assigned a pseudo-name comprising the top 5 most frequent words of the topic. The top 20 words were used to interpret and describe the meaning of the topics. Overall results per topic are visualised with bar charts and the evolution of topics and top words is visualised with line charts.

Using the $\theta$ statistic, the distribution of documents over topics is visualised to classify documents accoriding to their dominant topic and to show to what extent a document is specific to a topic or spread across topics.

Visualise topics with word clouds

To check how well the clustering is aligned with where the student carried out their exchange semester, we used a confusion matrix to visualize the relation between real values and values assigned through clustering.

```{r lda}
#| include: false

## K = 5 considering the 2 TUD semesters and the 3 exchange semesters combined 
emu_lda_5 <- LDA(emu_dtm, k = 5, method="Gibbs",
                 control = list(seed = 2023, iter = 500, verbose = 100))

# Extract beta and theta statistics from LDA model
beta_5 <- posterior(emu_lda_5)$terms
theta_5 <- posterior(emu_lda_5)$topics 

# Add pseudo-names to topics based on the top 5 words in each topic
topic_names_5 <- name_topics(beta_5)

```

# Results

<!-- Add intro paragraph -->

## Topics

```{r fig-topics-doc}
#| fig-cap: Topic probability distribution per document

# TODO Replace this with a table containing topics, documents, and theta
#      sorted by theta in decreasing order
# What is the dominant topic in each document?
topic_prop <- as.data.frame(theta_5)
colnames(topic_prop) <- paste0(paste0("Topic ", 1:5, ": "), topic_names_5)

topic_prop_viz <- 
  pivot_longer(data = data.frame(topic_prop,
                                 document = factor(rownames(topic_prop)),
                                 check.names = FALSE),
    cols = -document,
    names_to = "topic",
    values_to = "value") |> 
  group_by(document) |> 
  mutate(value_max = max(value),
         topic_max = topic[value == value_max])

p <- ggplot(data = topic_prop_viz, 
       aes(topic, value, fill = topic_max), 
       ylab = "proportion") + 
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  labs(title = "Theses as distributions over topics",
       subtitle = "Charts colored by the topic with the highest probability") +
  ylab("Probability") +
  scale_x_discrete(labels = paste0("Topic ", 1:5)) +
  coord_flip() +
  facet_wrap(~ factor(document, levels = rownames(topic_prop)), ncol = 10) +
  theme(legend.position = "bottom",
        legend.justification = "left",
        legend.direction = "vertical",
        axis.title.y = element_blank()) +
  scale_fill_viridis_d(name = "Dominant topic")

p_tab <- ggplotGrob(p)
# print(p_tab)

gtable_filter_remove <- function (x, name, trim = TRUE){
    matches <- !(x$layout$name %in% name)
    x$layout <- x$layout[matches, , drop = FALSE]
    x$grobs <- x$grobs[matches]
    if (trim) 
        x <- gtable_trim(x)
    x
}

select_grobs <- function() {
  x <- vector()
  for (i in 2:10) {
    for (j in 1:10) {
      x <- c(x, paste0("axis-l-", i, "-", j))
      x <- c(x, paste0("axis-b-", i, "-", j))
    }
  }
  x
}

p_filtered <- gtable_filter_remove(p_tab, name = select_grobs(), trim = FALSE)

library(grid)
grid::grid.newpage()
grid::grid.draw(p_filtered)

```

```{r topic-proportions-corpus}

# What is the mean probability of the documents being part of each topic?
topic_proportions <- get_topic_proportions(emu_lda_5)

# How many documents are with the highest probability in each topic?
counts_of_primary_topics <- count_docs_per_topic(emu_lda_5)

# Show summary table of mean probabilities and document counts per topic
topic_proportions |> 
  left_join(counts_of_primary_topics, by = "topic") |> 
  rename(Topic = topic,
         `Proportion per corpus` = proportion,
         `No. of documents with primary topic` = count)
```

The top 20 most frequently used words in the corpus (@fig-top-words-corpus), do not reflect the mean proportion ...

```{r fig-top-words-corpus}
# Which are the top n most frequently used words in all theses?
vis_top_words_per_corpus(lda = emu_lda_5, words = emu_words, top_n = 20)
```

```{r top-words-document}
# Which are the top n most frequently used words in each thesis?
get_top_words_per_document(data = emu_words, id_col = "ID", top_n = 3)
```

```{r top-words-topic}
vis_top_words_per_topic(data = emu_lda_5, top_n = 20)
```

```{r lda-vis}
#| eval: false

# TODO check if this can be included in the Shiny dashboard
# TODO remove from paper
# TODO how to show distances between topics in the paper?

vis_lda(phi = beta_5,
        theta = theta_5,
        dtm = emu_dtm)

```

The `tf-idf` statistic shows words that are the most important to one document in a collection of documents [@silge2017]. In this case, it shows high values for the names of thesis locations that tend to be high-frequency thesis-specific words.

```{r tf-idf}

## Calculate tf-idf statistic
emu_tf_idf <- emu_words |> 
  get_tf_idf(id_col = "ID", word_col = "word")

## Get top n words most specific to each thesis 
emu_tf_idf |> 
  group_by(ID) |> 
  top_n(5, tf_idf) |> 
  arrange(desc(tf_idf), .by_group = TRUE)

```

### Topic 1: `r topic_names_5[1]`

### Topic 2: `r topic_names_5[2]`

### Topic 3: `r topic_names_5[3]`

### Topic 4: `r topic_names_5[4]`

### Topic 5: `r topic_names_5[5]`

## The dynamics of thesis topics

```{r fig-topics-evolution}
#| eval: false

topic_prop_per_year <- theta_5[1:nrow(emu),]
colnames(topic_prop_per_year) <- topic_names_5

viz_df_with_year <-
  pivot_longer(
    data = data.frame(topic_prop_per_year,
                      document = factor(str_sub(
                        rownames(topic_prop_per_year), 1, 20)),
                      check.names = FALSE),
    cols = -document,
    names_to = "topic",
    values_to = "value") |> 
  left_join(mutate(emu, ID = as.factor(ID), "year" = grad_year), by = c("document" = "ID"))

# TODO change to discrete colors
# TODO represent this as a line chart
# Plot topic proportions per year
viz_df_with_year |> 
  ggplot(aes(x = factor(year), y = value, fill = topic)) + 
  geom_bar(stat = "identity", position = "fill") +
  scale_fill_viridis_d(name = "Topics") +
  xlab("Graduation year") +
  ylab("Proportion of topics") +
  labs(title = "Topic proportions per year")
```

```{r topic-model-gamma}
#| eval: false

# Extract documents as mixtures of topics from LDA model
emu_documents <- tidy(emu_lda_5, matrix = "gamma")

emu_documents |> 
  ggplot(aes(x = reorder(str_sub(document, 1, 50), gamma), y = gamma, fill = topic)) + 
  geom_bar(stat = "identity") + 
  coord_flip()

# TODO check if this is consistent with the results above
# TODO is gamma the same as theta ?!
emu_documents |>
  mutate(document = as.numeric(document)) |> 
  inner_join(emu |> 
               select(ID, grad_year), 
             by = c("document" = "ID")) |> 
  ggplot(aes(x = reorder(str_sub(document, 1, 50), gamma), y = gamma, fill = topic)) + 
  geom_bar(stat = "identity") + 
  coord_flip() + 
  facet_wrap(~ grad_year, ncol = 1, scales = "free", drop = TRUE)

# TODO check if this plot is useful and, if not, remove it
emu_documents |> 
  mutate(document = as.numeric(document)) |> 
  inner_join(emu |> 
               select(ID, grad_year), 
             by = c("document" = "ID")) |> 
  ggplot(aes(x = factor(grad_year), y = topic, fill = factor(topic))) +
  geom_col()
```

```{r}

top_words <- get_top_words_per_corpus(emu_words, top_n = 5, word_col = "word")

word_order <- emu_words |>
  group_by(word, grad_year) |>
  count(word, sort = TRUE) |>
  ungroup() |>
  arrange(grad_year,-n) |>
  group_by(grad_year) |>
  mutate(relative_frequency = n / sum(n) * 100) |>
  mutate(rank = round(rank(-n), 0)) |>
  ungroup() |>
  filter(word %in% top_words$word)

ggplot(word_order,
       aes(
         x = grad_year,
         y = relative_frequency,
         color = word,
         label = rank
       )) +
  # aes(fill = "white") +
  geom_smooth(lwd = 0.5) +
  # geom_point(size = 8, shape = 21, fill = "white", stroke = 1.2) +
  # geom_text(ggplot2::aes(x = grad_year), color = "black") +
  ggplot2::scale_color_viridis_d() +
  ggplot2::xlab("Year") + 
  ggplot2::ylab("Frequency (%)") +
  facet_wrap(~ word) +
  theme(legend.position = "none", 
        title = element_blank(),
        axis.text = element_text(size = 12),
        axis.title = element_text(size = 15))
```

As shown in @fig-topics-evolution, topics such as ... have gained importance over the years, while ... have lost prominence.

# Discussion

Although the focus of this study is on describing a fixed corpus though unsupervised machine learning, it can provide input into supervised predictive models.

Research directions...

# Conclusion

# Acknowledgements

<!-- The following line inserts a page break  -->

\newpage

# References

<!-- The following line ensures the references appear here for the MS Word or HTML output files, rather than right at the end of the document (this will not work for PDF files):  -->

::: {#refs}
:::

\newpage

### Colophon

This report was generated on `r Sys.time()` using the following computational environment and dependencies:

```{r}
#| label: colophon
#| cache: false

# which R packages and versions?
if ("devtools" %in% installed.packages()) devtools::session_info()
```

The current Git commit details are:

```{r}
# what commit is this file at? 
if ("git2r" %in% installed.packages() & git2r::in_repository(path = ".")) git2r::repository(here::here())  
```
