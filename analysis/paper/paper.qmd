---
title: "The Legacy of the European Post-Master in Urbanism at TU Delft: A Text Mining Approach"
author:
  - Claudiu Forgaci:
      correspondence: "yes"
      email: C.Forgaci@tudelft.nl
      orcid: 0000-0003-3218-5102
      institute: tud
institute:
  - tud:
      name: Delft University of Technology
      address: Julianalaan 134, 2628 BL, Delft, Zuid-Holland, The Netherlands
title-block-published: "Last updated"  
date: now
date-format: long
format: pdf
  # docx:
  #   reference-doc: "../templates/template.docx" # Insert path for the DOCX file
  # pdf:
  #   template: "../templates/plos_latex_template.tex"
execute:
  echo: false
  warning: false
  message: false
  comment: "#>"
  fig-path: "../figures/"
  fig-dpi: 600
filters:
  - ../templates/scholarly-metadata.lua
  - ../templates/author-info-blocks.lua
  - ../templates/pagebreak.lua
bibliography: references.bib
csl: "../templates/journal-of-archaeological-science.csl" # Insert path for the bib-style
abstract: |
  Prompted by the closure of the European post-Master of Urbanism (EMU) at TU Delft in 2021,
  this paper aims to describe the legacy of the EMU. To that end, it reports on the
  analysis of the graduation theses produced over 2007-2021 when the program was running.
  The text mining approach allows for a close scrutiny of the latent thematic patterns, 
  and evolutions thereof, found in the full texts of the theses. Results reveal # topics
  that are to a large extent consistent with the set-up of the program in two thematic 
  semesters at TU Delft and three possible exchange semester locations, each characterised 
  by a specific approach. The paper discusses the relevance of the findings for the
  state of the profession and education in the field. Set up as a reproducible research
  compendium accompanied by a published FAIR dataset of the theses and an interactive
  dashboard, the analysis presented can be both reproduced and applied to other datasets. 
keywords: |
  urbanism; education; post-master; text mining; topic modeling
highlights: |
  These are the highlights. 
---

<!-- The following code display values from the YAML header above. -->

Keywords: `r rmarkdown::metadata$keywords`

Highlights: `r rmarkdown::metadata$highlights`

<!-- Setup code, not shown in the rendered document. -->

```{r setup}
#| include: false

# TODO remove this section after all analysis has been translated into functions
# Load packages ----
## List packages to be loaded
packages <- c(
  "here",
  "tidyverse", "widyr", "readxl",   # Data manipulation and visualisation
  "furrr",                          # Parallel use of purrr::map() functions
  "tidytext", "SnowballC", "tm",    # Text processing and analysis
  "topicmodels", "LDAvis", "tsne",  # Topic modeling
  "tidygeocoder",                   # Working with spatial data
  "igraph", "ggraph", "DiagrammeR"  # Network analysis and visualization
)

## Load required packages, install them if needed
for (package in packages) {
  if (!require(package, character.only = TRUE)) {
    install.packages(package,  "http://cran.us.r-project.org")
  }
  library(package, character.only = TRUE)
}


# TODO should this be replaced with a call to `library(mintEMU)`?
# Load analysis functions ----
devtools::load_all(".")
```

```{r load-data}
#| include: false

# TODO merge into setup code chunk

# Load data for analysis
data(emu_metadata)  # Thesis metadata
data(emu_raw)       # Raw text data
data(emu_clean)     # Cleaned text data

# Join metadata and cleaned text data
emu <- emu_metadata |> 
  left_join(emu_raw, by = "ID") |> 
  left_join(emu_clean, by = "ID")


theses_all_per_year <- read_csv(file = here("analysis", "data", "derived_data", "emu-theses-per-year.csv"))

word_counts <- vector(mode = "numeric", length = nrow(emu))
for (i in 1:nrow(emu)) {
  word_counts[i] <- 
    tidytext::unnest_tokens(tbl = emu[i,], output = word, input = text_clean) |> nrow()
}

```

<!-- The actual document text starts here: -->

# Introduction {#intro}

The research presented in this paper was prompted by the closure of the European post-Master of Urbanism (EMU) at TU Delft. The EMU is an advanced program started jointly by TU Delft, KU Leuven, UPC Barcelona and Università IUAV di Venezia in 2005. The program distinguished itself internationally as a post-master open only to applicants with a prior MSc degree and practical experience in urbanism or a related field. With a combined research and design curriculum, it provided a bridge towards both advanced practice and research trajectories after graduation. All students followed an exchange semester at one of the other three program universities, allowing them to have a broad, international view on the field of urbanism. Although located in Europe and named "European", its geographic scope was global, especially at the stage of graduation, considering that a large majority of students were of non-European origin.

In 2021, TU Delft stepped out from the program, but its approach has remained imprinted on the TU Delft approach to urbanism. Its core topics Urban Region Networks (Semester 1) and Constructing Sustainable Urban Landscapes (Semester 2), as well as the influence of the partner universities, mostly IUAV Venice where most TU-Delft-hosted students spent their exchange semesters, are visible in the current MSc Urbanism and to some extent MSc Landscape Architecture tracks. There is general agreement on the considerable impact of the EMU on education and practice targeting sustainable urbanism, but why and how it is so remains implicit. Understanding what made the program impactful would be beneficial for targeted urbanism education and practice in response to current societal challenges. Having a close look at the assignments that were addressed by the program as well as research and design approaches used to tackle those topics throughout the years can provide valuable insight into what are the core, stable elements of urbanism practice, on the one hand, and how does the field need to adapt to changing societal challenges, on the other.

To describe the legacy of the EMU program, including the distinctive features of its didactic approach, this paper aims to reveal the main topics taught in it and how those topics had evolved over the years of the program. To that end, the research presented in this paper employed a text mining approach targeting the output of the EMU: 96 theses with an average of `r round(mean(word_counts, na.rm = TRUE), -3)` words produced over the years for the duration of the program between `r paste0(min(emu$graduation_year, na.rm = TRUE) - 2, "-", max(emu$graduation_year, na.rm = TRUE))`.

### Topic modeling {#topmod}

**The rise of computational social sciences** [@wallach2018] provides tools and methods to summarise large sets of data that would not be possible with human annotation [@blei2012]. At the same time, urbanism research, in general, and urbanism education studies, in particular, rarely use statistical methods such as topic modeling to analyse unstructured data. This is especially a missed opportunity considering that theses are representative outputs of educational programs that are increasingly available in digital repositories - the TU Delft repository was established in 2007, which is also the first graduation year of the EMU, and has been steadily growing since then. Patterns revealed in past education can potentially better inform future decisions in education in the field.

Although the corpus analysed in this study is relatively small, it is large enough to make a manually conducted quantitative analytical approach unfeasible.

Acknowledging that opportunity, **this paper leverages topic modeling**, a branch of Natural Language Processing, as an unsupervised machine learning method to reveal latent semantic structure and relationships between latent topics in a large document collection [@muchene2021]. Latent Dirichlet Allocation (LDA) is broadly adopted in topic modeling [@muchene2021] as it uses a *soft clustering algorithm* in which objects can be part of multiple clusters (mixed membership approach) as opposed to hard clustering algorithms (single membership approaches) [@rüdiger2022a]. LDA uses as input a bag-of-words (BOW) model, more generally called the vector space model (VSM), in which documents in a corpus are decomposed into two low-dimensional matrices for document-topic distribution and word-topic distribution, respectively. Although topic modeling in general automates the process of identifying topics through clustering, those topics need to be manually interpreted and labelled after the application of the algorithm [@rüdiger2022a]. Moreover, arguing for the right algorithm and a suitable number of clusters remains a challenge [@rüdiger2022].

In addition to describing the legacy of the EMU, **this paper showcases a reproducible research workflow**. Together with a published FAIR dataset, it forms a reproducible research compendium. The analyses presented in this paper were carried out in the free and open-source statistical programming software R [@rcoreteam2023]. The research compendium is available at \[ADD CITATION\], the data is published in th 4TU.ResearchData repository \[ADD CITATION\] and it is available for exploration in a Shiny dashboard at <http://mintemu_shiny.bk.tudelft.nl:3838/mintemu/>.

-   Give an overview of th predominant topics and their evolution over the years of the EMU program.

The following section describes the case study data, pre-processing steps and the topic modeling workflow. The results of the analysis are then summarised in response to two research questions. The paper concludes with a discussion on what the lessons learned from the EMU program mean for urbanism education and practice in general as well as on the broader challenges and opportunities of the text mining approach taken in the paper.

# Methods {#methods}

To describe the legacy of the EMU program this research answers two questions: What were the main topics addressed in the EMU program? And how did those topics evolve over the duration of the program? The paper then discusses to what extent those topics and evolution thereof were influenced by the assignments given to the students throughout their studies leading up to their thesis and by the exchange semester followed by the students. The research process is described in @fig-process-diagram.

```{r fig-process-diagram}
#| fig-cap: Text mining process
#| eval: false

mermaid("
graph LR
    A[Data collection]-->B[Pre-processing]
    B-->C[Information extraction]
    C-->B
    C-->D[Visualisation]
    D-->E[Results]
")
```

## Data collection {#datacol}

**The dataset consists of full texts and metadata of a subset of EMU theses written in English.** The full texts are available as PDF documents with unstructured layouts typical to an urbanism project in which text and various types of graphics are combined. The full-text documents were obtained either from the TU Delft Education Repository upon request (an API for downloading full-text data was not available at the time of data collection), or from the authors via email if not available on the repository. Metadata was downloaded from the TU Delft Education Repository or obtained from the EMU program coordination records when unavailable on the repository. Even though repository entries are openly accessible, licenses were not specified and thus it was unclear if the content of the theses can be mined and if the resulting data can be published. Considering the relatively small size of the corpus and the possibility of directly contacting the alumni network, permissions were requested individually. As a result, of the total of `r sum(theses_all_per_year$n_total)` theses presented in the EMU program, full-text documents were obtained for `r sum(theses_all_per_year$n_pdfs)` of which permissions were granted for `r nrow(emu)`. Only PDFs for which permission was obtained were included in the corpus. The final corpus contains at least one thesis for each year of graduation between `r min(emu$graduation_year)`-`r max(emu$graduation_year)` as shown in @tbl-data-selection. @fig-map-theses shows the geographic spread of the theses included in the analysis.

```{r tbl-data-selection}
#| tbl-cap: Overview of theses included in the analysis
#| tbl-cap-location: top

theses_all_per_year |> 
  select(grad_year, n_total, n_permissions) |> 
  mutate(grad_year = as.factor(grad_year),
         `% of all theses` = scales::label_percent()(round(n_permissions / n_total, 4))) |>
  bind_rows(data.frame(grad_year = "", 
                       n_permissions = nrow(emu), 
                       n_total = sum(theses_all_per_year$n_total),
                       `% of all theses` = scales::label_percent()(nrow(emu) / sum(theses_all_per_year$n_total)),
                       check.names = FALSE)) |> 
  filter(!is.na(grad_year)) |> 
  rename(`No. theses` = n_total,
         `Graduation year` = grad_year,
         `No. theses included` = n_permissions) |>
  kableExtra::kable(align = "lrrr")

```

```{r fig-map-theses}
#| fig-cap: "Location of EMU theses included in the analysis"

# Show thesis locations on world map
visualize_thesis_locations(emu)
```

## Pre-processing {#prepro}

For each full text, henceforth referred to as "document", the following cleaning steps were performed:

1.  Text was extracted from the original PDF documents with the the PyMuPDF Python package. During extraction, the front matter (cover page, colophon, table of contents, acknowledgements) and back matter (references, acknowledgements, appendices) were excluded. The page numbers used to exclude the front matter and back matter were manually identified and used as input for page selection during text extraction. The extracted text was stored in the `text_raw` column.
2.  The extracted text was cleaned in R of line breaks, hyphenation, occurrences of the thesis title, author name, and page numbers. The cleaned text data was added to each thesis in a new column `text_clean`.
3.  From the metadata only the title, graduation year, exchange semester and location of the case studied in the theses were kept. Column names were shortened and updated to follow the naming conventions used in the dataset.

The resulting dataset consists of the id, `title`, `grad_year`, `exch_sem`, `loc`, `lat`, `lon`, `text_raw` and `text_clean`, as shown in @tbl-data-variables.

```{r tbl-data-variables}

# TODO make sure that the columns reported in the text match those included in the dataset

tibble::tibble(Column = names(emu), 
               Type = purrr::map(emu, class),
               Completeness = 
                 purrr::map(emu, is.na) |> 
                 purrr::map(mean) |> 
                 purrr::map(\(x){1 - as.numeric(x)}) |> 
                 purrr::map(scales::label_percent())) |>
  kableExtra::kable()

```

Both the raw and cleaned data are published in the 4TU.ResearchData repository [@forgaci2023].

## Information extraction

<!-- Add intro to this section -->

Information extraction as a process of extracting information from unstructured data.

### Tokenisation

<!-- [Add to vignette:] Word-level tokenisation was applied with the `unnest_tokens()` function of the `tidytext` package. This was preferred over other tokenisers as it produced results that work seamlessly with other `tidyverse` tools used in the analysis. `unnest_tokens()` removes punctuation, converts to lower-case, and separate all words into individual tokens. -->

Word-level tokenisation was applied on the corpus. Bigrams such as "public space" and trigrams such as "socio-spatial segregation" identified with high frequency (at least 20 on a corpus level) during EDA were concatenated by replacing white spaces and hyphens with underscores. This way, they could be included alongside words in the topic model.

S**top words, i.e., common words or words that do not add much meaning, were removed**. Global, subject-specific and document stop words [@hvitfeldt2021a] were differentiated. As all theses were written in English, the pre-made "snowball" lexicon was used to remove global stop words. <!-- We tried to create a custom global stop word list on the entire corpus, by removing high-frequency words, starting with 20 words and increasing by 10 until reaching words that are not appropriate as stop words, but no significant improvement was observed in the result. --> We manually constructed a list of subject-specific stop words based on our domain knowledge. Words that are commonly used to structure a theses, such as "preface", "contents", and "introduction", were added as subject-specific stop words as well. Finally, titles and author names were removed as document stop words. In the next pre-processing step, we normalised the resulting words, that is, reduced them to their canonical dictionary forms, the resulting words were lemmatised [@hvitfeldt2021a]. Lemmatisation was preferred over stemming as the former tends to be more meaningful and less degrading than the latter.

```{r eval=FALSE}
ent1 <- spacyr::entity_extract(spacyr::spacy_parse(emu$text_clean[1:13]))

ent1 |> 
  filter(entity_type == "LOC") |> 
  pull(entity) |> 
  unique()

ent2 <- ent1 |> 
  mutate(entity = tolower(entity)) |> 
  group_by(entity) |> 
  mutate(bign = length(unique(doc_id))) |> 
  ungroup() |> 
  count(doc_id, entity, bign) |> 
  filter(bign > 3, nchar(entity) > 4)

dtm <- tidytext::cast_sparse(data = ent2,
                             row = doc_id,
                             column = entity,
                             value = n)

lda <- text2vec::LDA$new(n_topics = 50) 
fit <- lda$fit_transform(dtm, progressbar = F)
tm_summary <- # incomplete code

```

```{r eval=FALSE}
# Find locations
entity::location_entity(head_text(emu$text_clean[15], sub_end = 50000))
```

```{r tokenize}
# Unnest tokens (words, bigrams and trigrams) and remove words of one and two letters
emu_words <- tokenize(emu)
```

Finally, additional checks were made to the document in search for previously undetected stop words. Short words of one and two letters could be safely removed, while three-letter words were kept, as many of those, including "map" and "low", were meaningful and occurring in high frequency.

### Document-term matrix

```{r dtm}
# Prepare document-term matrix as input for LDA model
emu_dtm <- convert_to_dtm(emu_words, 
                          id_col = "ID", 
                          word_col = "word", 
                          min_term_freq = 5)
```

The topic model requires a document-term matrix (DTM) as input. To reduce the size of the DTM, words with a frequency lower than 5 were excluded.

### Topic model

Similar to clustering on numeric data, topic modelling is a method of unsupervised classification of topics found in a collection of documents [@silge2017]. We used the Latent Dirichlet Allocation (LDA) algorithm for topic modeling. LDA is a Bayesian generative topic model which assumes that each topic is a distribution over words and each document is a distribution over topics [@blei2012; @grün2011; @blei2003]. This means that each document is a mixture of topics and each topic is a mixture of words. The probability of a term being part of a topic is given by the $\beta$ (beta) statistic, while the probability of a document being part of a topic is given by the $\theta$ (theta) statistic.

The corpus (D) consists of a collection of M = `r nrow(emu)` documents with a total number of N = `r length(emu_words$word)` words and a vocabulary of V = `r length(unique(emu_words$word))` words. After the exclusion of words with a frequency lower than 5, the vocabulary was reduced to V = `r ncol(emu_dtm)`.

A key tuning parameter in an LDA model is the number of clusters K. A K with a smaller value will yield more general topics whereas a K with a high value will result in more specific topics. Hence, finding a suitable value entails a trade-off between interpretability and specificity. This is very much dependent on the data at hand and the research question. Given the small size of the corpus and the way the program was structured, the value of K = 5 was determined qualitatively considering the input received by the students in the two thematic semesters at TU Delft and the three alternative exchange semesters, each with a very distinct approach:  

- TU Delft, Fall semester: Urban region networks
- TU Delft, Spring semester: Constructing the sustainable city
- IUAV Venice, exchange semester:
- KU Leuven, exchange semester:
- UPC Barcelona, exchange semester:

To check how well the clustering is aligned with where the student carried out their exchange semester, we used a confusion matrix to visualize the relation between real values and values assigned through clustering.

Get top 20 words in each topic to understand what it is about

Visualise topics with word clouds

```{r lda-k-5}
#| message: false

## K = 5 considering the 2 TUD semesters and the 3 exchange semesters combined 
emu_lda_5 <- LDA(emu_dtm, k = 5, method="Gibbs",
                 control = list(seed = 2023, iter = 500, verbose = 25))

# Extract beta and theta statistics from LDA model
beta_5 <- posterior(emu_lda_5)$terms
theta_5 <- posterior(emu_lda_5)$topics 

# Add pseudo-names to topics based on the top 5 words in each topic
topic_names_5 <- name_topics(beta_5)

```

```{r fig-topics-per-doc}
#| fig-cap: Topic probability distribution per document

# TODO Replace this with a table containing topics, documents, and theta
#      sorted by theta in decreasing order
# What is the dominant topic in each document?
N = nrow(theta_5)
topic_prop_examples <- theta_5[1:N,]
colnames(topic_prop_examples) <- topic_names_5

vizDataFrame <- pivot_longer(
  data = data.frame(topic_prop_examples, 
                    document = factor(str_sub(rownames(topic_prop_examples), 1, 20))),
  cols = -document,
  names_to = "topic",
  values_to = "value") |> 
  group_by(document) |> 
  mutate(value_max = max(value),
         topic_max = topic[value == value_max])
ggplot(data = vizDataFrame, aes(topic, value, fill = topic_max), ylab = "proportion") + 
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  coord_flip() +
  facet_wrap(~ as.numeric(document), ncol = 10) +
  theme(legend.position = "bottom") +
  scale_fill_viridis_d(name = "Topic") +
  labs(title = "Theses as distributions over topics",
       subtitle = "Charts colored by the highest probability")

```

```{r topics-per-corpus}

# What are the most probable topics in the entire collection?
topic_proportions <- get_topic_proportions(emu_lda_5)
# paste0(round(topic_proportions$proportion, 2), ": ", topic_proportions$topic)

# How many documents are with the highest probability in each topic?
counts_of_primary_topics <- count_docs_per_topic(emu_lda_5)
# paste0(counts_of_primary_topics$count, ": ", topic_proportions$topic)

topic_proportions |> 
  left_join(counts_of_primary_topics, by = "topic") |> 
  rename(Topic = topic,
         `Proportion per corpus` = proportion,
         `No. of documents with primary topic` = count)
```

```{r}
#| eval: false

topic_to_filter <- 1  # set manually
# or have it selected by a term in the topic name (e.g. 'airport')
topic_to_filter <- grep('water', topic_names_5)[1] 
topic_threshold <- 0.2
selected_document_indices <- which(theta_5[, topic_to_filter] >= topic_threshold)
filtered_corpus <- emu[selected_document_indices,]
# show length of filtered corpus
nrow(filtered_corpus)
```

```{r}
#| eval: false

N = nrow(emu)
topic_prop_per_year <- theta_5[1:N,]
colnames(topic_prop_per_year) <- topic_names_5

viz_df_with_year <-
  pivot_longer(
    data = data.frame(topic_prop_examples,
                      document = factor(str_sub(
                        rownames(topic_prop_examples), 1, 20))),
    cols = -document,
    names_to = "topic",
    values_to = "value") |> 
  left_join(mutate(emu, ID = as.factor(ID), "year" = grad_year), by = c("document" = "ID"))

# TODO change to discrete colors
# TODO try representing this as a line chart
# Plot topic proportions per year
viz_df_with_year |> 
  ggplot(aes(x = factor(year), y = value, fill = topic)) + 
  geom_bar(stat = "identity", position = "fill") +
  scale_fill_viridis_d(name = "Topics") +
  xlab("Graduation year") +
  ylab("Proportion of topics") +
  labs(title = "Topic proportions per year")
```

```{r lda-vis}
#| eval: false

# TODO check if this can be included in the Shiny dashboard
# TODO remove from paper
# TODO how to show distances between topics in the paper?

visualise_lda(phi = beta_5, 
              theta = theta_5, 
              dtm = emu_dtm)

```

```{r topic-model-beta}

# Extract topics as mixtures of terms from LDA model
emu_topics <- tidy(emu_lda_5, matrix = "beta")

# The top n most common words in each topic
emu_top_terms <- get_top_words_per_topic(data = emu_topics, top_n = 20)

# TODO create visualisation function
emu_top_terms |> 
  ggplot(aes(reorder(term, beta), beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

# TODO can this be generalised to more than two topics?

beta_spread <- emu_topics |> 
  mutate(topic = paste0("topic", topic)) |> 
  spread(topic, beta) |> 
  filter(topic1 > 0.001, topic2 > .001) |> 
  mutate(log_ratio = log2(topic2/topic1))

beta_spread %>%
  group_by(direction = log_ratio > 0) %>%
  slice_max(abs(log_ratio), n = 20) %>% 
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(log_ratio, term)) +
  geom_col() +
  labs(x = "Log2 ratio of beta in topic 2 / topic 1", y = NULL)
```

```{r topic-model-gamma}
#| eval: false

# Extract documents as mixtures of topics from LDA model
emu_documents <- tidy(emu_lda_5, matrix = "gamma")

emu_documents |> 
  ggplot(aes(x = reorder(str_sub(document, 1, 50), gamma), y = gamma, fill = topic)) + 
  geom_bar(stat = "identity") + 
  coord_flip()

# TODO check if this is consistent with the results above
# TODO is gamma the same as theta ?!
emu_documents |>
  mutate(document = as.numeric(document)) |> 
  inner_join(emu |> 
               select(ID, grad_year), 
             by = c("document" = "ID")) |> 
  ggplot(aes(x = reorder(str_sub(document, 1, 50), gamma), y = gamma, fill = topic)) + 
  geom_bar(stat = "identity") + 
  coord_flip() + 
  facet_wrap(~ grad_year, ncol = 1, scales = "free", drop = TRUE)

# TODO check if this plot is useful and, if not, remove it
emu_documents |> 
  inner_join(emu |> 
               select(ID, grad_year), 
             by = c("document" = "ID")) |> 
  ggplot(aes(x = factor(gradyear), y = topic, fill = factor(topic))) +
  geom_col()
```

# Results

## Overall patterns

The tf-idf statistic shows that...

## Topics

## The dynamics of thesis topics

Topics such as ... have gained importance over the years, while ... have lost prominence.

```{r top-words-document}
#| eval: false

# Which are the top n most frequently used words in each thesis?
get_top_words_per_document(data = emu_words, id_col = "ID", top_n = 10)
```

```{r top-words-corpus}

# Which are the top n most frequently used words in all theses?
terms <- as.data.frame(posterior(emu_lda_5)$terms)
rownames(terms) <- topic_names_5
terms <- terms |> 
  mutate(topic = rownames(terms)) |> 
  pivot_longer(-topic,
               names_to = "term",
               values_to = "prob") |> 
  group_by(term) |> 
  mutate(max_topic = topic[which.max(prob)]) |> 
  filter(topic == max_topic) |> 
  ungroup()

emu_words_topics <- emu_words |> 
  left_join(terms, by = c("word" = "term"))

n = 20
get_top_words_per_corpus(data = emu_words, top_n = n) |>
  left_join(terms, by = c("word" = "term")) |> 
  ggplot(aes(reorder(word, n), n)) +
  geom_col(aes(fill = max_topic)) +
  geom_text(aes(label = n), size = 2, hjust = 1.1) +
  coord_flip() +
  xlab("Word") +
  ylab("Frequency") +
  labs(title = paste0("Top ", n, " most used words in the corpus of theses")) +
  theme_minimal() +
  theme(panel.grid = element_blank())
```

A frequency count of ngrams reveals collocations like "public space" or "socio-spatial segregation" across the corpus. To include these in the topic model, we concatenated them with an underscore, so that they are considered one word during the word-level tokenisation. Only bigrams and trigrams, that is two- and three-word collocations, were extracted from the corpus.

```{r bigram-tfidf}

emu_bigrams <- emu |> 
  get_ngrams(n = 2, id_col = "ID", text_col = "text_clean", stem = TRUE)

bigram_counts <- emu_bigrams |> 
  count(w_1, w_2, sort = TRUE)

bigram_tf_idf <- emu_bigrams |> 
  count(ID, ngram) |> 
  bind_tf_idf(ngram, ID, n) |> 
  arrange(desc(tf_idf))

bigram_tf_idf
```

```{r trigram-tfidf}

emu_trigrams <- emu |> 
  get_ngrams(n = 3, id_col = "ID", text_col = "text_raw")

trigram_counts <- emu_trigrams |> 
  count(w_1, w_2, w_3, sort = TRUE)

trigram_tf_idf <- emu_trigrams |> 
  count(ID, ngram) |> 
  bind_tf_idf(ngram, ID, n) |> 
  arrange(desc(tf_idf))

trigram_tf_idf

trigram_tf_idf |> arrange(desc(n))
```

```{r}
## Show network of bigrams
bigram_graph <- bigram_counts |> 
  filter(n > 100) |> 
  graph_from_data_frame()

bigram_graph

set.seed(2023)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
  geom_node_point(color = "lightblue", linewidth = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

## Higgh frequency bigrams ----

emu_bigrams_top <- emu_bigrams |>
  group_by(ID) |>
  count(ngram, sort = TRUE) |>
  slice_max(n, n = 20) |>        ### top_n is superseded; better use slice_max instead
  # filter(n > 50)                ### filter words with a minimum count
  ungroup() |>
  mutate(bigram = reorder(ngram, n))

emu_bigrams_top_all <- emu_bigrams |>
  count(ngram, sort = TRUE) |>
  slice_max(n, n = 20) |>        ### top_n is superseded; better use slice_max instead
  # filter(n > 50)                ### filter words with a minimum count
  mutate(ngram = reorder(ngram, n))

emu_bigrams_top |>
  filter(ID == unique(ID)[1:20]) |> 
  ggplot(aes(x = ngram, y = n, fill = ID)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(y = "Count",
       x = "Unique words",
       title = "Count of unique words found in thesis",
       subtitle = "Stop words were removed from the list") +
  facet_wrap( ~ ID, scales = "free", ncol = 2) +
  theme(legend.position = "none") 
```

The `tf-idf` statistic shows words that are the most important to one document in a collection of documents [@silge2017]. In this case, it shows high values for the names of thesis locations that tend to be high-frequency thesis-specific words.

```{r tf-idf}

## Calculate tf-idf statistic
emu_tf_idf <- emu_words |> 
  get_tf_idf(id_col = "ID", word_col = "word")

## Get top n words most specific to each thesis 
emu_tf_idf |> 
  group_by(ID) |> 
  top_n(5, tf_idf) |> 
  arrange(desc(tf_idf), .by_group = TRUE)

```

```{r}

# Preview term frequency distributions in a subset of theses
emu_words_count <- emu_words |> 
  count(ID, word, sort = TRUE) |> 
  ungroup()

emu_words_total <- emu_words_count |> 
  group_by(ID) |> 
  summarise(total = sum(n))

emu_words_count <- left_join(emu_words_count, emu_words_total)

ggplot(emu_words_count |> 
         filter(ID == unique(ID)[1:6]),
       aes(n/total, fill = ID)) +
  geom_histogram(show.legend = FALSE, binwidth = 0.0001) +
  xlim(NA, 0.0009) +
  facet_wrap(~ ID, ncol = 3, scales = "free_y")
```

```{r}
# Examine Zipf's law for the EMU theses
## Calculate rank and term frequency
freq_by_rank <- emu_words_count |> 
  group_by(ID) |> 
  mutate(rank = row_number(),
         `term frequency` = n/total)

## Visualise Zipf's law
freq_by_rank |> 
  ggplot(aes(rank, `term frequency`, color = ID)) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()

rank_subset <- freq_by_rank |> 
  filter(rank < 500,
         rank > 10)

lm(log10(`term frequency`) ~ log10(rank), data =  rank_subset)

freq_by_rank |> 
  ggplot(aes(rank, `term frequency`, color = ID)) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
  geom_abline(intercept = -1.44, slope = -0.7, color = "gray50", linetype = 2) +
  scale_x_log10() +
  scale_y_log10()
```

```{r}
#| eval: false

## Visualise the top n words with the highest tf-idf values for each of a subset of theses
emu_words_count |> 
  get_tf_idf(id_col = "ID", word_col = "word") |> 
  filter(ID == unique(ID)[1:2]) |>  # Keep only the first 6 theses
  arrange(desc(tf_idf)) |> 
  mutate(word = factor(word, levels = rev(unique(word)))) |> 
  group_by(ID) |> 
  top_n(15) |> 
  ungroup() |> 
  ggplot(aes(word, tf_idf, fill = ID)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~ ID,  ncol = 3, scales = "free") +
  coord_flip()
```

### Word embeddings

Describe word embeddings ...

```{r word-embeddings-pmi}

# To what extent do words co-occur with other words?
tidy_pmi <- get_pmi(emu, id_col = "ID", text_col = "text_clean")
```

The high-dimensional sparse matrix of word features is projected into reduced, 100-dimensional set of features.

```{r word-embeddings-svd}

# Deter                                                                        mine word vectors using SVD, a method for dimensionality reduction
tidy_word_vectors <- tidy_pmi |> 
  widely_svd(
    item1, item2, pmi,
    nv = 100, maxit = 1000
  )
```

What terms should be checked with the function below?

```{r word-embeddings-nn}

# Calculate nearest neighbours of a given word
tidy_word_vectors |> 
  nearest_neighbors("water")
```

The first \# components... what do they show?

```{r word-embeddings-vis}

# The first n components with top words found in the corpus of theses
visualise_first_n_components(tidy_word_vectors, 24, 12)
```

```{r doc-embeddings}

tidy_words <- emu |> 
  select(ID, text_clean) |> 
  unnest_tokens(word, text_clean) |> 
  add_count(word) |> 
  filter(n >= 50) |> 
  select(-n)
  
# Summarise word embeddings into document embeddings for modeling purposes
word_matrix <- tidy_words |> 
  count(ID, word) |> 
  cast_sparse(ID, word, n)

embedding_matrix <- tidy_word_vectors |> 
  cast_sparse(item1, dimension, value)

doc_matrix <- word_matrix %*% embedding_matrix

dim(doc_matrix)
```

# Discussion

Although the focus of this study is on describing a fixed corpus though unsupervised machine learning, it can provide input into supervised predictive models.

# Conclusion

# Acknowledgements

<!-- The following line inserts a page break  -->

\newpage

# References

<!-- The following line ensures the references appear here for the MS Word or HTML output files, rather than right at the end of the document (this will not work for PDF files):  -->

::: {#refs}
:::

\newpage

### Colophon

This report was generated on `r Sys.time()` using the following computational environment and dependencies:

```{r}
#| label: colophon
#| cache: false

# which R packages and versions?
if ("devtools" %in% installed.packages()) devtools::session_info()
```

The current Git commit details are:

```{r}
# what commit is this file at? 
if ("git2r" %in% installed.packages() & git2r::in_repository(path = ".")) git2r::repository(here::here())  
```
