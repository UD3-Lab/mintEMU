---
title: "The Legacy of the European Post-Master in Urbanism at TU Delft: A Text Mining Approach"
author:
  - name: Claudiu Forgaci
    correspondence: "yes"
    email: C.Forgaci@tudelft.nl
    orcid: 0000-0003-3218-5102
    affiliations: 
      - ref: tud
affiliations:
  - id: tud
    name: Delft University of Technology
    address: Julianalaan 134, 2628 BL, Delft, Zuid-Holland, The Netherlands
title-block-published: "Last updated"  
date: now
date-format: long
format:
  html: default
  # docx:
  #   reference-doc: ../templates/template.docx
execute:
  echo: false
  warning: false
  message: false
  comment: "#>"
  fig-path: "../figures/"
  fig-dpi: 600
filters:
  - ../templates/scholarly-metadata.lua
  - ../templates/author-info-blocks.lua
  - ../templates/pagebreak.lua
bibliography: references.bib
csl: "../templates/journal-of-archaeological-science.csl" # Insert path for the bib-style
abstract: |
  This paper aims to describe the legacy of the European post-Master of Urbanism 
  (EMU) that ran between 2005 and 2021 at TU Delft. To that end, it reports on the
  analysis of the graduation theses produced over the duration of the program.
  The text mining approach allows for a close scrutiny of the latent thematic patterns, 
  and evolutions thereof, found in the full texts of the theses. Results reveal 5 topics
  that are to a large extent consistent with the set-up of the program in two thematic 
  semesters at TU Delft and three possible exchange semester locations, each characterised 
  by a specific approach. The paper discusses the relevance of the findings for the
  state of the profession and education in the field. Set up as a research
  compendium accompanied by a published FAIR dataset and an interactive dashboard, 
  the research is fully reproducible and enables further analysis. 
keywords: |
  urbanism; education; post-master; text mining; topic modeling
highlights: |
  These are the highlights. 
---

<!-- The following code can be used to show values from the YAML header in DOCX output. -->

<!-- Keywords: `r rmarkdown::metadata$keywords` -->

<!-- Highlights: `r rmarkdown::metadata$highlights` -->

<!-- Setup code, not shown in the rendered document. -->

```{r}
#| label: setup
#| include: false

# Load packages ----
library(tidyverse)

# Load analysis functions ----
## TODO should this be replaced with a call to `library(mintEMU)`?
devtools::load_all(".")

# Load data ----
data(emu_metadata)  # Thesis metadata
data(emu_raw)       # Raw full text data
data(emu_clean)     # Cleaned full text data

# Join metadata and cleaned text data ----
emu <- emu_metadata |> 
  left_join(emu_raw, by = "ID") |> 
  left_join(emu_clean, by = "ID") |> 
  select(-c(grad_sem, title, subtitle, abstract)) |> 
  rename(title = full_title) |> 
  select("ID", "title", everything())

# Get counts per corpus ----
theses_all_per_year <- read_csv(file = here::here("analysis", "data", "derived_data", "emu-theses-per-year.csv"))
word_counts <- get_word_counts(emu, text_col = "text_clean")
```

<!-- The actual document text starts here: -->

## Introduction {#intro}

The research presented in this paper was prompted by the closure of the European post-Master of Urbanism (EMU) at TU Delft in 2021. The EMU is an advanced program started jointly by TU Delft, KU Leuven, UPC Barcelona and Università IUAV di Venezia in 2005. The program distinguished itself internationally as an advanced post-master open only to applicants with a prior MSc degree and practical experience in urbanism or a related field. With a combined research and design curriculum, it provided a bridge towards both advanced practice and research trajectories after graduation. All students followed an exchange semester at one of the other three program universities, allowing them to have a broad, international view on the field of urbanism. Although located in Europe and named "European", its geographic scope was global, visible especially at the stage of graduation, considering that a large majority of students were of non-European origin and chose to work on a case from their home location.

In 2021, TU Delft left the program, but its approach has remained imprinted on the TU Delft approach to urbanism. Its core topics Urban Region Networks (Semester 1) and Constructing Sustainable Urban Landscapes (Semester 2), as well as the influence of the partner universities—mostly IUAV Venice where most TU-Delft-hosted students spent their exchange semesters—are visible in the current MSc Urbanism and, to some extent, MSc Landscape Architecture tracks. There is general agreement on the considerable impact of the EMU on education and practice targeting sustainable urbanism, but why and how it is so remains implicit. Understanding what made the program impactful would be beneficial for targeted urbanism education and practice in response to current societal challenges. Having a close look at the assignments that were addressed by the program as well as research and design approaches used to tackle those topics throughout the years can provide valuable insight into what are the core, stable elements of urbanism practice, on the one hand, and how does the field need to adapt to changing societal challenges, on the other.

To describe the legacy of the EMU program, including the distinctive features of its didactic approach, this paper aims to reveal the main topics taught in it and how those topics had evolved over the years of the program. To that end, the research presented in this paper employed a text mining approach targeting the output of the EMU program: 96 theses with a rounded average of `r round(mean(word_counts, na.rm = TRUE), -3) |> format(big.mark=",")` words produced over the years for the duration of the program between `r paste0(min(emu$grad_year, na.rm = TRUE) - 2, "-", max(emu$grad_year, na.rm = TRUE))`. Although the corpus of theses is relatively small, it is large enough to make a manually conducted quantitative analytical approach unfeasible.

## A text mining approach {#textmin}

The rise of computational social sciences [@wallach2018] provides tools and methods to summarise large sets of data that would not be possible with human annotation [@blei2012]. Often such data is available in unstructured textual form and formats that are not easily machine-readable, such as Adobe PDF or Microsoft Word documents [@benchimol2022]. While quantitative text analysis have been widely adopted in computational social sciences such as linguistics, communication, and political science [@benchimol2022], urbanism research, in general, and urbanism education studies, in particular, rarely use statistical methods such as topic modeling to analyse unstructured data. This is especially a missed opportunity considering that theses are representative outputs of educational programs that are increasingly available in digital repositories—the TU Delft repository was established in 2007, which is also the first graduation year of the EMU, and has been steadily growing since then. Patterns revealed in past education can potentially better inform future decisions in education in the field.

Acknowledging that opportunity, this paper leverages topic modeling, a branch of Natural Language Processing, as an unsupervised machine learning method to reveal latent semantic structure and relationships between latent topics in a large document collection [@muchene2021]. Latent Dirichlet Allocation (LDA) is broadly adopted in topic modeling [@muchene2021] as it uses a soft clustering algorithm in which objects can be part of multiple clusters (mixed membership approach) as opposed to hard clustering algorithms (single membership approaches) [@rüdiger2022]. LDA uses as input a bag-of-words (BOW) model, more generally called the vector space model (VSM), in which documents in a corpus are decomposed into two low-dimensional matrices for document-topic distribution and word-topic distribution, respectively. Although topic modeling in general automates the process of identifying topics through clustering, those topics need to be manually interpreted and labelled after the application of the algorithm [@rüdiger2022]. Moreover, arguing for the right algorithm and a suitable number of clusters remains a challenge [@rüdiger2022].

In addition to describing the legacy of the EMU, this paper showcases a reproducible research workflow. Together with a published FAIR dataset, it forms a reproducible research compendium. The analyses presented in this paper were carried out in the free and open-source statistical programming software R [@rcoreteam2023]. The research compendium is available at \[ADD CITATION\], the data is published in th 4TU.ResearchData repository [@forgaci2023] and it is available for exploration in a Shiny dashboard at <http://mintemu_shiny.bk.tudelft.nl:3838/mintemu/>.

The following section describes the case study data, pre-processing steps and the topic modeling workflow. The results of the analysis are then summarised in response to two research questions. The paper concludes with a discussion on what the lessons learned from the EMU program mean for urbanism education and practice in general as well as on the broader challenges and opportunities of the text mining approach taken in the paper.

# Methods {#methods}

To describe the legacy of the EMU program this research answers two questions: What were the main topics addressed in the EMU program? And how did those topics evolve over the duration of the program? The paper then discusses to what extent those topics and evolution thereof were influenced by the assignments given to the students throughout their studies leading up to their thesis and by the exchange semester followed by the students. The research process is described in @fig-process-diagram.

::: {#fig-process-diagram}
```{mermaid}
graph LR
    A[Data collection]-->B[Pre-processing]
    B-->C[Text mining]
    C-->D[Visualisation]
    D-->B
    D-->E[Results]
```

Text mining process
:::

## Data collection {#datacol}

```{r}
#| label: data-collection
#| include: false

n_theses_all <- sum(theses_all_per_year$n_total)
n_theses_pdf <- sum(theses_all_per_year$n_pdfs)
n_theses <- nrow(emu)

start_year <- min(emu$grad_year)
end_year <- max(emu$grad_year)

theses_overview <- theses_all_per_year |> 
  select(grad_year, n_total, n_permissions) |> 
  mutate(grad_year = as.factor(grad_year),
         `% of all theses per period` = scales::label_percent()(round(n_permissions / n_total, 4))) |>
  bind_rows(data.frame(grad_year = "2007-2021", 
                       n_permissions = nrow(emu), 
                       n_total = sum(theses_all_per_year$n_total),
                       `% of all theses per period` = scales::label_percent()(nrow(emu) / sum(theses_all_per_year$n_total)),
                       check.names = FALSE)) |> 
  filter(!is.na(grad_year)) |> 
  rename(`No. theses` = n_total,
         `Graduation year` = grad_year,
         `No. theses included` = n_permissions) 

perc_theses_included <- tail(theses_overview, 1)[ncol(theses_overview)] |> pull()
```

The dataset consists of full texts and metadata of a subset of EMU theses written in English. The full texts are available as PDF documents with unstructured layouts typical to an urbanism project in which text and various types of graphics are combined @fig-pdf-example. The full-text documents were obtained either from the TU Delft Education Repository upon request, or from the authors via email if not available on the repository. Metadata was downloaded from the TU Delft Education Repository or obtained from the EMU program coordination records if unavailable on the repository. Even though repository entries are openly accessible, licenses were not specified and thus it was unclear if the content of the theses can be mined and if the resulting data can be published. Considering the relatively small size of the corpus and the possibility of directly contacting the alumni network, permissions were requested individually. As a result, of the total of `r n_theses_all` theses presented in the EMU program, full-text documents were obtained for `r n_theses_pdf` of which permissions were granted for `r n_theses`, that is, `r perc_theses_included` of all theses. Only PDFs for which permission was obtained were included in the corpus. The final corpus contains at least one thesis for each year of graduation between `r start_year`-`r end_year` as shown in @tbl-data-selection. @fig-map-theses shows the geographic spread of the theses included in the analysis.

::: {#fig-pdf-example}
![](pdf-example.png)

Example of facing pages from one of the theses in the corpus.
:::

```{r}
#| label: tbl-data-selection
#| tbl-cap: Overview of theses included in the analysis
#| tbl-cap-location: top

theses_overview |>
  kableExtra::kable(align = "lrrr")
```

```{r}
#| label: fig-map-theses
#| fig-cap: "Location of EMU theses included in the analysis"
#| warning: false

# Show thesis locations on world map
visualize_thesis_locations(emu)
```

## Pre-processing {#prepro}

For each full text, henceforth referred to as "document", the following cleaning steps were performed:

1.  Text was extracted from the original PDF documents with the the PyMuPDF Python package [@pymupdf/2024]. During extraction, the front matter (cover page, colophon, table of contents, acknowledgements) and back matter (references, acknowledgements, appendices) were excluded. The page numbers used to exclude the front matter and back matter were manually identified and used as input for page selection during text extraction. The extracted text was stored in the `text_raw` column of the dataset.
2.  The extracted text was cleaned in R of line breaks, hyphenation, occurrences of the thesis title, author name, and page numbers. The cleaned text data was added to each thesis in a new column `text_clean`.
3.  From the metadata only the title, graduation year, exchange semester and location of the case studied in the theses were kept. Column names were shortened and updated to follow the naming conventions used in the dataset.

The resulting dataset consists of `ID`, `title`, `grad_year`, `loc`, `lat`, `lon`, `text_raw` and `text_clean`, as shown in @tbl-data-variables.

```{r}
#| label: tbl-data-variables
#| tbl-cap: Variables included in the dataset.
#| tbl-cap-location: top

tibble::tibble(Column = names(emu), 
               Type = purrr::map(emu, class),
               Completeness = 
                 purrr::map(emu, is.na) |> 
                 purrr::map(mean) |> 
                 purrr::map(\(x){1 - as.numeric(x)}) |> 
                 purrr::map(scales::label_percent())) |>
  kableExtra::kable(align = "llr")

```

Both the raw and cleaned data are published in the 4TU.ResearchData repository [@forgaci2023].

## Text mining {#infoext}

<!-- TODO Add intro to this section -->

Text mining is a process of extracting information from unstructured data following the steps of tokenisation, construction of a Document-Term Matrix (DTM), fitting a topic model and visualisation of the results.

### Tokenization

Word-level tokenization was applied on the corpus. In addition, bigrams such as "public space" and trigrams such as "socio-spatial segregation" identified with high frequency (at least 20 on a corpus level) during exploratory data analysis (EDA) were concatenated by replacing white spaces and hyphens with underscores. This way, they could be included alongside words in the topic model.

Stop words, i.e., common words or words that do not add much meaning, were removed. Global, subject-specific and document stop words [@hvitfeldt2021a] were differentiated. As all theses were written in English, the pre-made "snowball" lexicon was used to remove global stop words [@porter2001]. We manually constructed a list of subject-specific stop words based on our domain knowledge. Words that are commonly used to structure a theses, such as "preface", "contents", and "introduction", were added as subject-specific stop words as well. Finally, titles and author names were removed as document stop words. In the next pre-processing step, we normalised the resulting words, that is, reduced them to their canonical dictionary forms, the resulting words were lemmatised [@hvitfeldt2021a]. Lemmatisation was preferred over stemming as the former tends to be more meaningful and less degrading than the latter.

```{r}
#| label: tokens

# Unnest tokens (words, bigrams and trigrams) and remove words of one and two letters
words <- tokenize(emu)
```

Finally, additional checks were made to the document in search for previously undetected stop words. Short words of one and two letters could be safely removed, while three-letter words were kept, as many of those, such as "map", were meaningful and occurring in high frequency.

### Document-term matrix

```{r}
#| label: dtm
# Prepare document-term matrix as input for LDA model
dtm <- convert_to_dtm(words, 
                          id_col = "ID", 
                          word_col = "word", 
                          min_term_freq = 5)
```

<!-- TODO Elaborate or merge with previous section -->

The tokenized corpus was then converted into a document-term matrix (DTM), which is the format needed as input for the topic model. To reduce the size of the DTM, words with a corpus-level frequency lower than 5 were excluded.

### Topic model

Similar to clustering on numeric data [@rüdiger2022], topic modelling is a method of unsupervised classification of topics found in a collection of documents [@silge2017]. We used Latent Dirichlet Allocation (LDA), a generative probabilist topic model which assumes that each topic is a distribution over words and each document is a distribution over topics [@blei2012; @grün2011; @blei2003]. This means that each document is a mixture of topics and each topic is a mixture of words. The probability of a word being part of a topic is given by the $\beta$ (beta) statistic, while the probability of a document being part of a topic is given by the $\theta$ (theta) statistic.

The corpus ($D$) consists of a collection of M = `r nrow(emu)` documents with a total number of N = `r length(words$word) |> format(big.mark=",")` words and a vocabulary of V = `r length(unique(words$word)) |> format(big.mark=",")` words. After the exclusion of words with a frequency lower than 5, the vocabulary was reduced to V = `r ncol(dtm) |> format(big.mark=",")`.

A key tuning parameter (i.e., a manually specified parameter) in an LDA model is the number of clusters $K$. A $K$ with a smaller value will yield more general topics whereas a $K$ with a high value will result in more specific topics. Hence, finding a suitable value entails a trade-off between interpretability and specificity<!-- add reference here -->. This is very much dependent on the data at hand and the research question. Given the small size of the corpus and the way the program was structured, the value of $K = 5$ was determined qualitatively considering the input received by the students in the two thematic semesters at TU Delft and the three alternative exchange semesters, each with a very distinct approach, as described in @tbl-semesters.

| University    | Semester        | Topic                                            |
|-----------------|-----------------|--------------------------------------|
| TU Delft      | Fall semester   | Urban region networks                            |
|               | Spring semester | Constructing the sustainable city                |
| IUAV Venice   | Fall semester   | Territories of dispersion: Situations, Scenarios |
| KU Leuven     | Fall semester   |                                                  |
| UPC Barcelona | Fall semester   | Urban Transformations                            |

: Topics covered by each EMU semester at TU Delft and in the exchange locations. {#tbl-semesters}

Each topic was assigned a pseudo-name comprising the top 5 most frequent words of the topic. The top 20 words were used to interpret and describe the meaning of the topics qualitatively. Overall results per topic were visualised with bar charts and the evolution of topics and top words was visualised with line charts.

Using the $\theta$ statistic, the distribution of documents over topics is visualised to classify documents according to their dominant topic and to show to what extent a document is specific to a topic or spread across topics.

Visualise topics with word clouds

To check how well the clustering is aligned with where the student carried out their exchange semester, we used a confusion matrix to visualize the relation between real values and values assigned through clustering.

```{r}
#| label: lda
#| include: false

# TODO transform this into a function

## K = 5 considering the 2 TUD semesters and the 3 exchange semesters combined 
lda <- topicmodels::LDA(dtm, k = 3, method="Gibbs",
                        control = list(seed = 2023, iter = 500, verbose = 100))

# Extract beta and theta statistics from LDA model
beta <- modeltools::posterior(lda)$terms
theta <- modeltools::posterior(lda)$topics 

# Add pseudo-names to topics based on the top 5 words in each topic
topic_names_5 <- name_topics(beta)

```

# Results

<!-- Add intro paragraph -->

## Topics

```{r}
#| label: fig-topics-doc
#| fig-cap: Topic probability distribution per document

# TODO Replace this with a table containing topics, documents, and theta
#      sorted by theta in decreasing order
# What is the dominant topic in each document?
topic_prop <- as.data.frame(theta)
colnames(topic_prop) <- paste0(paste0("Topic ", 1:length(topic_names_5), ": "), topic_names_5)

topic_prop_viz <- 
  pivot_longer(data = data.frame(topic_prop,
                                 document = factor(rownames(topic_prop)),
                                 check.names = FALSE),
    cols = -document,
    names_to = "topic",
    values_to = "value") |> 
  group_by(document) |> 
  mutate(value_max = max(value),
         topic_max = topic[value == value_max])

p <- ggplot(data = topic_prop_viz, 
       aes(topic, value, fill = topic_max), 
       ylab = "proportion") + 
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  labs(title = "Theses as distributions over topics",
       subtitle = "Charts colored by the topic with the highest probability") +
  ylab("Probability") +
  scale_x_discrete(labels = paste0("Topic ", 1:5)) +
  coord_flip() +
  facet_wrap(~ factor(document, levels = rownames(topic_prop)), ncol = 10) +
  theme(legend.position = "bottom",
        legend.justification = "left",
        legend.direction = "vertical",
        axis.title.y = element_blank(),
        strip.text = element_text(size = 5),
        strip.background = element_rect(size = 1)) +
  scale_fill_viridis_d(name = "Dominant topic")


p_tab <- ggplotGrob(p)
# print(p_tab)

gtable_filter_remove <- function (x, name, trim = TRUE){
    matches <- !(x$layout$name %in% name)
    x$layout <- x$layout[matches, , drop = FALSE]
    x$grobs <- x$grobs[matches]
    if (trim) 
        x <- gtable_trim(x)
    x
}

select_grobs <- function() {
  x <- vector()
  for (i in 2:10) {
    for (j in 1:10) {
      x <- c(x, paste0("axis-l-", i, "-", j))
      x <- c(x, paste0("axis-b-", i, "-", j))
    }
  }
  x
}

p_filtered <- gtable_filter_remove(p_tab, name = select_grobs(), trim = FALSE)

library(grid)
grid::grid.newpage()
grid::grid.draw(p_filtered)

```

```{r}
#| label: topic-proportions-corpus

# What is the mean probability of the documents being part of each topic?
topic_proportions <- get_topic_proportions(lda)

# How many documents are with the highest probability in each topic?
counts_of_primary_topics <- count_docs_per_topic(lda)

# Show summary table of mean probabilities and document counts per topic
topic_proportions |> 
  left_join(counts_of_primary_topics, by = "topic") |> 
  rename(Topic = topic,
         `Proportion per corpus` = proportion,
         `No. of documents with primary topic` = count)
```

The top 20 most frequently used words in the corpus (@fig-top-words-corpus), do not reflect the mean proportion ...

```{r}
#| label: fig-top-words-corpus

# Which are the top n most frequently used words in all theses?
n <- 20
vis_top_words_per_corpus(lda = lda, words = words, top_n = n)
```

```{r}
#| label: top-words-document

# Which are the top n most frequently used words in each thesis?
n <- 3
get_top_words_per_document(data = words, id_col = "ID", top_n = n)
```

```{r}
#| label: top-words-topic

# Which are the top n most frequently used words in each thesis?
n <- 20
vis_top_words_per_topic(data = lda, top_n = n)
```

```{r}
theta |> 
  as_tibble() |> 
  mutate(ID = as.numeric(rownames(theta))) |> 
  pivot_longer(cols = `1`:ncol(theta),
               names_to = "topic",
               values_to = "prob") |> 
  group_by(ID) |> 
  mutate(max_prob = max(prob, na.rm = FALSE),
         max_topic = if_else(round(prob, digits = 7) == round(max_prob, digits = 7), topic, NA)) |> 
  ungroup() |> 
  select(ID, max_prob, max_topic) |> 
  drop_na() |> 
  left_join(emu) |> 
  select(ID, max_prob, max_topic, title, text_clean) |> 
  # group_by(max_topic) |> 
  # filter(max_prob == max(max_prob)) |> 
  arrange(max_topic, desc(max_prob)) |> 
  select(max_topic, max_prob, ID, title) |> 
  kableExtra::kable()

# words_repr <- words |> 
#   left_join(repr_theses) |> 
#   drop_na()
# 
# set.seed(2023)
# wordcloud::wordcloud(words_repr |> filter(ID == unique(words_repr$ID)[3]) |> pull(word), min.freq = 60)
```

The `tf-idf` statistic shows words that are the most important to one document in a collection of documents [@silge2017]. In this case, it shows high values for the names of thesis locations that tend to be high-frequency thesis-specific words.

```{r}
#| label: tf-idf

## Calculate tf-idf statistic
tf_idf <- words |> 
  get_tf_idf(id_col = "ID", word_col = "word")

## Get top n words most specific to each thesis 
tf_idf |> 
  group_by(ID) |> 
  top_n(5, tf_idf) |> 
  arrange(desc(tf_idf), .by_group = TRUE)

```

### Topic 1: `r topic_names_5[1]`

**People and places:** *community land house develop region*

Community, social, ... local scale

### Topic 2: `r topic_names_5[2]`

**Landscapes:** *water landscape system river land*

The most dominant topic highlights the systemic, landscape-driven component of the program. This topic was central in the Spring semester at TU Delft.

### Topic 3: `r topic_names_5[3]`

**Spatial morphology:** *block street build athens house*

This topic is predominantly morphological... density... plot...

The theses with the highest probabilities in this cluster...

### Topic 4: `r topic_names_5[4]`

**Integrated projects:** *park build main house community*

### Topic 5: `r topic_names_5[5]`

**Regional networks:** *territory land rural potential region*

This topic represents territorial challenges that go beyond the urban to consider

### Integrated projects

Theses that have a broader distribution across topics have an integrative character. Do those theses have a relatively higher frequency of the term "integrate"? Also, Topics \# and \# share the dominant term "system" as they both represent systemic approaches

## The dynamics of thesis topics

```{r}
#| label: fig-topics-evolution
#| eval: false

topic_prop_per_year <- theta[1:nrow(emu),]
colnames(topic_prop_per_year) <- topic_names_5

viz_df_with_year <-
  pivot_longer(
    data = data.frame(topic_prop_per_year,
                      document = factor(str_sub(
                        rownames(topic_prop_per_year), 1, 20)),
                      check.names = FALSE),
    cols = -document,
    names_to = "topic",
    values_to = "value") |> 
  left_join(mutate(emu, ID = as.factor(ID), "year" = grad_year), by = c("document" = "ID"))

# TODO represent this as a line chart
# Plot topic proportions per year
viz_df_with_year |> 
  ggplot(aes(x = factor(year), y = value, fill = topic)) + 
  geom_bar(stat = "identity", position = "fill") +
  scale_fill_viridis_d(name = "Topics") +
  xlab("Graduation year") +
  ylab("Proportion of topics") +
  labs(title = "Topic proportions per year")
```

```{r}
#| label: topic-model-gamma
#| eval: false

# Extract documents as mixtures of topics from LDA model
documents <- tidy(lda, matrix = "gamma")

documents |> 
  ggplot(aes(x = reorder(str_sub(document, 1, 50), gamma), y = gamma, fill = topic)) + 
  geom_bar(stat = "identity") + 
  coord_flip()

# TODO check if this is consistent with the results above
# TODO is gamma the same as theta ?!
documents |>
  mutate(document = as.numeric(document)) |> 
  inner_join(emu |> 
               select(ID, grad_year), 
             by = c("document" = "ID")) |> 
  ggplot(aes(x = reorder(str_sub(document, 1, 50), gamma), y = gamma, fill = topic)) + 
  geom_bar(stat = "identity") + 
  coord_flip() + 
  facet_wrap(~ grad_year, ncol = 1, scales = "free", drop = TRUE)

# TODO check if this plot is useful and, if not, remove it
documents |> 
  mutate(document = as.numeric(document)) |> 
  inner_join(emu |> 
               select(ID, grad_year), 
             by = c("document" = "ID")) |> 
  ggplot(aes(x = factor(grad_year), y = topic, fill = factor(topic))) +
  geom_col()
```

```{r}

top_words <- get_top_words_per_corpus(words, top_n = 5, word_col = "word")

word_order <- words |>
  group_by(word, grad_year) |>
  count(word, sort = TRUE) |>
  ungroup() |>
  arrange(grad_year,-n) |>
  group_by(grad_year) |>
  mutate(relative_frequency = n / sum(n) * 100) |>
  mutate(rank = round(rank(-n), 0)) |>
  ungroup() |>
  filter(word %in% top_words$word)

ggplot(word_order,
       aes(
         x = grad_year,
         y = relative_frequency,
         color = word,
         label = rank
       )) +
  # aes(fill = "white") +
  geom_smooth(lwd = 0.5) +
  # geom_point(size = 8, shape = 21, fill = "white", stroke = 1.2) +
  # geom_text(ggplot2::aes(x = grad_year), color = "black") +
  ggplot2::scale_color_viridis_d() +
  ggplot2::xlab("Year") + 
  ggplot2::ylab("Frequency (%)") +
  facet_wrap(~ word) +
  theme(legend.position = "none", 
        title = element_blank(),
        axis.text = element_text(size = 12),
        axis.title = element_text(size = 15))
```

As shown in @fig-topics-evolution, topics such as ... have gained importance over the years, while ... have lost prominence.

-   show mean length of theses over time

-   discuss why theses tend to be longer over time: more frictionless workflows, pre-made layouts, more focus on research

# Discussion

The relevance of the findings for the state of the profession and urbanism education...

What are the core, stable elements of urbanism practice and how does the field need to adapt to changing societal challenges?

The topics can be useful for searching...

The number of topics...

Although the focus of this study is on describing a fixed corpus though unsupervised machine learning, it can provide input into supervised predictive models.

Research directions...

-   what is generalisable to urbanism theses in general

    -   increasing length is probably the case for all theses

# Conclusion {#conclusion}

# Acknowledgements

I would like to acknowledge the invaluable contribution of Aleksandra Wilczynska to the development of this paper as a reproducible research compendium as part of DCC support provided by the TU Delft library. I would also like to thank former EMU coordinators Birgit Hausleitner and Luiz De Carvalho Filho for their support in collecting the metadata and data used in this paper, as well as to Bjarne van der Drift and Greta Samulionytė for their assistance in completing the metadata.

<!-- The following line inserts a page break  -->

\newpage

# References

<!-- The following line ensures the references appear here for the MS Word or HTML output files, rather than right at the end of the document (this will not work for PDF files):  -->

::: {#refs}
:::

\newpage

### Colophon

This paper was generated on `r Sys.time()` using the following computational environment and dependencies:

```{r}
#| label: colophon
#| cache: false

# which R packages and versions?
if ("devtools" %in% installed.packages()) devtools::session_info()
```

The current Git commit details are:

```{r}
#| label: last-commit
# what commit is this file at? 
if ("git2r" %in% installed.packages() & git2r::in_repository(path = ".")) git2r::repository(here::here())  
```
