---
title: "The Legacy of the European Post-Master in Urbanism at TU Delft: A Text Mining Approach"
author:
  - Claudiu Forgaci:
      correspondence: "yes"
      email: C.Forgaci@tudelft.nl
      orcid: 0000-0003-3218-5102
      institute: tud
institute:
  - tud:
      name: Delft University of Technology
      address: Julianalaan 134, 2628 BL, Delft, Zuid-Holland, The Netherlands
title-block-published: "Last updated"  
date: now
date-format: long
format: 
  docx:
    reference-doc: "../templates/template.docx" # Insert path for the DOCX file
execute:
  echo: true
  warning: false
  message: false
  comment: "#>"
  fig-path: "../figures/"
  fig-dpi: 600
filters:
  - ../templates/scholarly-metadata.lua
  - ../templates/author-info-blocks.lua
  - ../templates/pagebreak.lua
bibliography: references.bib
csl: "../templates/journal-of-archaeological-science.csl" # Insert path for the bib-style
abstract: |
  Text of abstract
keywords: |
  urbanism; education; post-master; text mining
highlights: |
  These are the highlights. 
---

<!-- With the following code you can access and display values from the yml header above. -->

Keywords: `r rmarkdown::metadata$keywords`

Highlights: `r rmarkdown::metadata$highlights`

<!-- The actual document text starts here: -->

```{r setup}
#| echo: false
#| warning: false

# Load packages ----
## List packages to be loaded
packages <- c(
  "here",                                        # Managing paths
  "tidyverse", "reshape2", "widyr",              # Data manipulation and visualisation
  "furrr",                                       # Parallel use of purrr::map() functions
  "tidytext", "SnowballC", "tm", "topicmodels",  # Text processing and analysis
  "tidygeocoder",                                # Working with spatial data
  "igraph", "ggraph"                             # Network analysis and visualization
)

## Load required packages, install them if needed
for (package in packages) {
  if (!require(package, character.only = TRUE)) {
    install.packages(package,  "http://cran.us.r-project.org")
  }
  library(package, character.only = TRUE)
}


# Load analysis functions ----
devtools::load_all(".")


# Read thesis metadata ----
data_path <- here("analysis", "data", "raw_data")
pdf_names <- dir(data_path, pattern = "*.pdf")

## Read metadata for all theses
all_theses <-
  readxl::read_xlsx(path = here(data_path, "theses-all.xlsx"))

## Read only metadata for theses with PDF available
emu_theses <- 
  read_csv(here(data_path, "theses-metadata.csv")) |>
  mutate(text = "") |>
  filter(!is.na(pdf_via))

pdf_paths <- here(data_path, emu_theses$file_name)
```

```{r get-pdf-text}
#| echo: false
#| cache: true

data_path <- here("analysis", "data", "derived_data", "emu_theses_with_text_and_location.csv")

if (!file.exists(data_path)) {
  # On the first run, this function might take extra time as a Python environment needs to be set up
  emu_theses$text <- convert_pdf_text(pdf_paths)
  
  # Add latitude and longitude information to theses data frame
  emu_theses <- geocode_thesis_locations(emu_theses)

  # Write thesis dataframe to file
  write_csv(emu_theses, data_path)
} else {
  emu_theses$text <- read_csv(data_path)$text
  emu_theses$longitude <- read_csv(data_path)$longitude
  emu_theses$latitude <- read_csv(data_path)$latitude
}

```

```{r clean-text}
#| echo: false

emu_theses$text_clean <- emu_theses$text |>
  clean_basic() 
  
```

# Introduction

The research presented in this paper was prompted by the closure of the European post-Master of Urbanism (EMU) of the Department of Urbanism at the Faculty of Architecture and the Built Environment, TU Delft. The EMU was an advanced master ran jointly by TU Delft, KU Leuven, UPC Barcelona and Universit√† IUAV di Venezia. <!-- Complete paragraph about the program -->

Distinctive features of the program:  
- as a postmaster, it only accepted applicants with a prior MSc degree and experience in practice
- provides a bridge towards PhD-level research
- all students had an exchange semester at one of the other three program universities



In order to describe the legacy of the EMU program, including the distinctive features of its didactic approach, this paper aims to reveal the main topics taught in it and how those topics had evolved through the years of the program. To that end, we employed a text mining approach in which we analysed its output: `r nrow(all_theses)` theses with an average of `r mean(str_length(emu_theses$text_clean), na.rm = TRUE)` words produced over the years for the duration of the program between `r paste0(min(all_theses$grad_year, na.rm = TRUE), "-", max(all_theses$grad_year, na.rm = TRUE))`. 

# Methods

Research questions:

-   What were the main topics addressed in the EMU program?

-   How did those topics evolve throughout the duration of the program between 2007-2021?

-   To what extent were the topics and evolution thereof influenced by the assignments given to the students throughout their studies leading up to their thesis?

-   To what extent were the topics and evolution thereof influenced by the exchange semester followed by the students?
    - Use LDA clustering with 3 classes (corresponding to the three exchange universities) to see how well the clustering is aligned with where the student carried out their exchange semester. Use a confusion matrix to visualize the relation between real values and values assigned through clustering. 

## Data collection

The theses are available in PDF format with complex layouts typical to an urbanism project in which text and various types of graphics are combined. With the exception of a bilingual English-Spanish thesis, all thesis were written in English.

A total number of `r nrow(emu_theses)` out of `r nrow(all_theses)` were analysed.

```{r fig-map-theses}
#| warning: false 
#| echo: false
#| fig-cap: "Location of EMU theses included in the analysis"

# Show thesis locations on world map
visualize_thesis_locations(emu_theses)
```

@fig-map-theses shows the geographic spread of the theses included in the analysis.

## Data analysis

### Tokenisation

Word-level tokenisation was applied with the `unnest_tokens()` function of the `tidytext` package. This was preferred over other tokenisers as it produced results that work seamlessly with other `tidyverse` tools used in the analysis.

### Stop words

For analyses in which word-level tokenisation was chosen, stop words, i.e., common words or words that do not add much meaning, were removed in the data preparation stage. Global, subject-specific and document stop words [@hvitfeldt2021a] were differentiated. As all theses were written in English, the pre-made "snowball" lexicon was used to remove global stop words. <!-- We tried to create a custom global stop word list on the entire corpus, by removing high-frequency words, starting with 20 words and increasing by 10 until reaching words that are not appropriate as stop words, but no significant improvement was observed in the result. --> We manually constructed a list of subject-specific stop words based on our domain knowledge. Words that are commonly used to structure a theses, such as "preface", "contents", and "introduction", were added as subject-specific stop words as well. Finally, titles and author names were removed as document stop words.

```{r stop-words}

emu_theses$text_clean <- emu_theses$text_clean |>
  str_remove_all(find_meta_stopwords(emu_theses)) |> 
  str_remove_all(urbanism_stopwords(
    add_stopwords = c("emu", "european postgraduate master in urbanism", "tu delft", "ku leuven", "upcs barcelona", "iuav venice"))) |>
  str_remove_all(thesis_stopwords(add_stopwords = c("advisor", "prof")))

emu_theses_words <- emu_theses |> 
  select(title, text_clean) |> 
  unnest_tokens(output = word, input = text_clean) |>   # remove punctuation, convert to lowercase, separate all words
  anti_join(tidytext::stop_words, by = "word")

# TODO can we use this instead of the short_words function?
emu_theses_words <- emu_theses_words |> 
  filter(nchar(word) > 4)

```

Additional checks were made to the document in search for previously undetected stop words. The checks included identifying words consisting of four or less characters that extensively used in the text corpus. 

```{r}
#| label: short-stop-words

#short_words(emu_theses$text_clean, scope = 'corpus')
short_words(emu_theses_words$word)

```

Stemming is a pre-processing step that needs to be carefully thought through, as it might either degrade topic modeling or produce no meaningful results [@hvitfeldt2021a].

```{r stemming}

emu_theses_words %>% 
  mutate(stem = wordStem(words = word, 
                         language = "porter")) |>  # Implement the Porter stemming algorithm for English provided by the SnowballC package
  count(stem, sort = TRUE)

```

## Topic model

Similar to clustering on numeric data, topic modeling is a method of unsupervised classification of topics found in a group of documents [@silge2017]. We will use the Latent Dirichlet Allocation (LDA) algorithm for topic modeling. In LDA, each document is a mixture of topics and each topic is a mixture of words. The probability of a term being part of a topic is given by the $\beta$ (beta) statistic, while the probability of a document being part of a topic is given by the $\gamma$ (gamma) statistic.

```{r topic-model-lda}

# Prepare document-term matrix as input for LDA model
emu_dtm <- convert_to_dtm(emu_theses_words, "title", "word")

# Create LDA model
emu_lda <- LDA(emu_dtm, k = 2, control = list(seed = 2023))

```



```{r topic-model-beta}

# Extract topics as mixtures of terms from LDA model
emu_topics <- tidy(emu_lda, matrix = "beta")

# The top n most common words in each topic
emu_theses_top_terms <- get_top_words_per_topic(emu_topics, 20)

# TODO create visualisation function
emu_theses_top_terms |> 
  ggplot(aes(reorder(term, beta), beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

# TODO can this be generalised to more than two topics?
beta_spread <- emu_theses_topics |> 
  mutate(topic = paste0("topic", topic)) |> 
  spread(topic, beta) |> 
  filter(topic1 > 0.001, topic2 > .001) |> 
  mutate(log_ratio = log2(topic2/topic1))

beta_spread %>%
  group_by(direction = log_ratio > 0) %>%
  slice_max(abs(log_ratio), n = 20) %>% 
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(log_ratio, term)) +
  geom_col() +
  labs(x = "Log2 ratio of beta in topic 2 / topic 1", y = NULL)
```


```{r topic-model-gamma}

# Extract documents as mixtures of topics from LDA model
emu_documents <- tidy(emu_lda, matrix = "gamma")

emu_documents |> 
  ggplot(aes(x = reorder(str_sub(document, 1, 50), gamma), y = gamma, fill = topic)) + 
  geom_bar(stat = "identity") + 
  coord_flip()

emu_documents |> 
  inner_join(emu_theses |> 
               select(title, graduation_year), 
             by = c("document" = "title")) |> 
  ggplot(aes(x = reorder(str_sub(document, 1, 50), gamma), y = gamma, fill = topic)) + 
  geom_bar(stat = "identity") + 
  coord_flip() + 
  facet_wrap(~ graduation_year, ncol = 1, scales = "free", drop = TRUE)

emu_documents |> 
  inner_join(emu_theses |> 
               select(title, graduation_year), 
             by = c("document" = "title")) |> 
  ggplot(aes(x = factor(graduation_year), y = topic, fill = factor(topic))) +
  geom_col()
```


# Results

```{r}
# Which are the top n most frequently used words in each thesis?
get_top_words_per_document(data = emu_theses_words, top_n = 3)
```

```{r}
# Which are the top n most frequently used words in all theses?
get_top_words_per_corpus(data = emu_theses_words, top_n = 20)
```

<!-- ```{r} -->

<!-- # Which theses contain "fabric" in the top 20 most frequently used words? -->

<!-- emu_theses_top_20 %>%  -->

<!--   filter(word == "fabric") -->

<!-- emu_theses_top_20 %>% -->

<!--   slice(1:40) %>%  -->

<!--   ggplot(aes(x = reorder(word, n), y = n, fill = title)) + -->

<!--   geom_col() + -->

<!--   xlab(NULL) + -->

<!--   coord_flip() + -->

<!--   labs(y = "Count", -->

<!--        x = "Unique words", -->

<!--        title = "Count of unique words found in thesis", -->

<!--        subtitle = "Stop words were removed from the list") + -->

<!--   facet_wrap( ~ title, scales = "free", ncol = 2) -->

<!-- ``` -->

```{r}
#| label: ngrams

## Bigrams
emu_theses_bigrams <- emu_theses |> 
  get_ngrams(n = 2, title_col = "title", text_col = "text_clean", stem = TRUE)

spacyr::spacy_parse(emu_theses_bigrams$w_1)

bigram_counts <- emu_theses_bigrams |> 
  count(w_1, w_2, sort = TRUE)

bigram_stem_counts <- emu_theses_bigrams |> 
  count(w_1_stem, w_2_stem, sort = TRUE)

bigram_counts
bigram_stem_counts

## Trigrams
emu_theses_trigrams <- emu_theses |> 
  get_ngrams(n = 3, title_col = "title", text_col = "text")

trigram_counts <- emu_theses_trigrams |> 
  count(w_1, w_2, w_3, sort = TRUE)

trigram_counts

## Bigram tf-idf
bigram_tf_idf <- emu_theses_bigrams |> 
  count(title, ngram) |> 
  bind_tf_idf(ngram, title, n) |> 
  arrange(desc(tf_idf))

bigram_tf_idf

## Trigram tf-idf
trigram_tf_idf <- emu_theses_trigrams |> 
  count(title, ngram) |> 
  bind_tf_idf(ngram, title, n) |> 
  arrange(desc(tf_idf))

trigram_tf_idf

## Show network of bigrams
bigram_graph <- bigram_counts |> 
  filter(n > 100) |> 
  graph_from_data_frame()

bigram_graph

set.seed(2023)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
  geom_node_point(color = "lightblue", linewidth = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

## Higgh frequency bigrams ----

emu_theses_bigrams_top <- emu_theses_bigrams |>
  group_by(title) |>
  count(ngram, sort = TRUE) |>
  slice_max(n, n = 20) |>        ### top_n is superseded; better use slice_max instead
  # filter(n > 50)                ### filter words with a minimum count
  ungroup() |>
  mutate(bigram = reorder(ngram, n))

emu_theses_bigrams_top_all <- emu_theses_bigrams |>
  count(ngram, sort = TRUE) |>
  slice_max(n, n = 20) |>        ### top_n is superseded; better use slice_max instead
  # filter(n > 50)                ### filter words with a minimum count
  mutate(ngram = reorder(ngram, n))

emu_theses_bigrams_top |>
  filter(title == unique(title)[1:10]) |> 
  ggplot(aes(x = ngram, y = n, fill = title)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(y = "Count",
       x = "Unique words",
       title = "Count of unique words found in thesis",
       subtitle = "Stop words were removed from the list") +
  facet_wrap( ~ title, scales = "free", ncol = 2) +
  theme(legend.position = "none") 
```

The `tf-idf` statistic shows words that are the most important to one document in a collection of documents [@silge2017]. In this case, it shows high values for the names of thesis locations that tend to be high-frequency thesis-specific words. 

```{r tf-idf}
## Calculate tf-idf statistic
emu_theses_tf_idf <- emu_theses_words |> 
  get_tf_idf(title_col = "title", word_col = "word")

## Get top n words most specific to each thesis 
emu_theses_tf_idf |> 
  group_by(title) |> 
  top_n(5, tf_idf) |> 
  arrange(desc(tf_idf), .by_group = TRUE)
```


```{r}
# Preview term frequency distributions in a subset of theses
ggplot(emu_theses_words_count |> 
         filter(title == unique(title)[1:6]),
       aes(n/total, fill = title)) +
  geom_histogram(show.legend = FALSE, binwidth = 0.0001) +
  xlim(NA, 0.0009) +
  facet_wrap(~ title, ncol = 3, scales = "free_y")
```


```{r}
# Examine Zipf's law for the EMU theses
## Calculate rank and term frequency
freq_by_rank <- emu_theses_words_count |> 
  group_by(title) |> 
  mutate(rank = row_number(),
         `term frequency` = n/total)

## Visualise Zipf's law
freq_by_rank |> 
  ggplot(aes(rank, `term frequency`, color = title)) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()

rank_subset <- freq_by_rank |> 
  filter(rank < 500,
         rank > 10)

lm(log10(`term frequency`) ~ log10(rank), data =  rank_subset)

freq_by_rank |> 
  ggplot(aes(rank, `term frequency`, color = title)) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
  geom_abline(intercept = -1.44, slope = -0.7, color = "gray50", linetype = 2) +
  scale_x_log10() +
  scale_y_log10()

## Visualise the top n words with the highest tf-idf values for each of a subset of theses
emu_theses_words_count |> 
  filter(title == unique(title)[1:6]) |> 
  arrange(desc(tf_idf)) |> 
  mutate(word = factor(word, levels = rev(unique(word)))) |> 
  group_by(title) |> 
  top_n(15) |> 
  ungroup() |> 
  ggplot(aes(word, tf_idf, fill = title)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~ title,  ncol = 3, scales = "free") +
  coord_flip()
```

### Word embeddings  

[Describe word embeddings ...]

```{r word-embeddings-pmi}
#| include: false

# To what extent do words co-occur with other words?
tidy_pmi <- get_pmi(emu_theses, title_col = "title", text_col = "text_clean")

```

The high-dimensional sparse matrix of word features is projected into reduced, 100-dimensional set of features.

```{r word-embeddings-svd}

# Determine word vectors using SVD, a method for dimensionality reduction
tidy_word_vectors <- tidy_pmi |> 
  widely_svd(
    item1, item2, pmi,
    nv = 100, maxit = 1000
  )

```

[What terms should be checked with the function below?]

```{r word-embeddings-nn}

# Calculate nearest neighbours of a given word
tidy_word_vectors |> 
  nearest_neighbors("resilience")

```

The first [?] components... [what do they show?]

```{r word-embeddings}

# The first n components with top words found in the corpus of theses
visualise_first_n_components(tidy_word_vectors, 24, 12)

```


```{r word-embeddings}

# Summarise word embeddings into document embeddings for modeling purposes
word_matrix <- tidy_emu_theses |> 
  count(title, word) |> 
  cast_sparse(title, word, n)

embedding_matrix <- tidy_word_vectors |> 
  cast_sparse(item1, dimension, value)

doc_matrix <- word_matrix %*% embedding_matrix

dim(doc_matrix)

doc_matrix

```



# Discussion

# Conclusion

# Acknowledgements

<!-- The following line inserts a page break  -->

\newpage

# References

<!-- The following line ensures the references appear here for the MS Word or HTML output files, rather than right at the end of the document (this will not work for PDF files):  -->

::: {#refs}
:::

\newpage

### Colophon

This report was generated on `r Sys.time()` using the following computational environment and dependencies:

```{r}
#| label: colophon
#| cache: false

# which R packages and versions?
if ("devtools" %in% installed.packages()) devtools::session_info()
```

The current Git commit details are:

```{r}
# what commit is this file at? 
if ("git2r" %in% installed.packages() & git2r::in_repository(path = ".")) git2r::repository(here::here())  
```
