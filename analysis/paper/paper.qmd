---
title: "The Legacy of the European Post-Master in Urbanism at TU Delft: A Text Mining Approach"
author:
  - Jane Doe:
      correspondence: "yes"
      email: janedoe@fosg.org
      orcid: 0000-0003-1689-0557
      institute: tud
  - John Q. Doe:
      email: johndoe@fosg.org
      orcid: 0000-0003-1689-0558
      institute: tud
institute:
  - tud:
      name: Delft University of Technology
      address: Julianalaan 134, 2628 BL, Delft, Zuid-Holland, The Netherlands
title-block-published: "Last updated"  
date: now
date-format: long
format: 
  docx:
    reference-doc: "../templates/template.docx" # Insert path for the DOCX file
execute:
  echo: true
  warning: false
  message: false
  comment: "#>"
  fig-path: "../figures/"
  fig-dpi: 600
filters:
  - ../templates/scholarly-metadata.lua
  - ../templates/author-info-blocks.lua
  - ../templates/pagebreak.lua
bibliography: references.bib
csl: "../templates/journal-of-archaeological-science.csl" # Insert path for the bib-style
abstract: |
  Text of abstract
keywords: |
  urbanism; education; post-master; text mining
highlights: |
  These are the highlights. 
---

<!-- With the following code you can access and display values from the yml header above. -->

Keywords: `r rmarkdown::metadata$keywords`

Highlights: `r rmarkdown::metadata$highlights`

<!-- The actual document text starts here: -->

```{r}
#| label: setup
#| echo: false
#| warning: false

# Load packages ----
## Managing paths
library(here)

## Data manipulation and visualisation
library(tidyverse)
library(reshape2)

## Text processing and analysis
library(tidytext)
library(SnowballC)
library(tm)
library(topicmodels)

## Working with spatial data
library(tidygeocoder)


# Load analysis functions ----
devtools::load_all(".")


# Read thesis metadata ----
## All theses
theses_all <-
  readxl::read_xlsx(path = here("analysis", "data", "raw_data", "theses-all.xlsx"))

## Theses with PDF available
data_path <- here("analysis", "data", "raw_data")
pdf_names <- dir(data_path, pattern = "*.pdf")

emu_theses <- 
  read_csv(here(data_path, "theses-metadata.csv")) %>%
  mutate(text = "") %>%
  filter(!is.na(pdf_via))
  
pdf_paths <- here(data_path, emu_theses$file_name)
```

```{r}
#| label: extract-pdf-text
#| echo: false
#| cache: true

emu_theses$text <- convert_pdf_text(pdf_paths)
```

```{r}
#| label: clean-text
#| echo: false

emu_theses$text_clean <- emu_theses$text |> 
  clean_basic() |> 
  str_remove_all(find_meta_stopwords(emu_theses) )  
```

```{r}
#| label: preview
#| include: false

# Preview theses
emu_theses %>% 
  arrange(graduation_year) %>% 
  pull(text_clean) %>% 
  str_sub(1, 500)

# Preview theses
hist(str_length(emu_theses$text_clean))

Encoding(emu_theses$text_clean)
```

# Introduction

The research presented in this paper was prompted by the closure of the European post-Master of Urbanism (EMU) of the Department of Urbanism at the Faculty of Architecture and the Built Environment, TU Delft. The EMU was an advanced master ran jointly by TU Delft, KU Leuven, UPC Barcelona and Universit√† IUAV di Venezia. <!-- Complete paragraph about the program -->

In order to describe the legacy of the EMU program, including the distinctive features of its didactic approach, this paper aims to reveal the main topics taught in it and how those topics had evolved through the years of the program. To that end, we employed a text mining approach in which we analysed its output: `r nrow(theses_all)` theses with an average of `r mean(str_length(emu_theses$text_clean), na.rm = TRUE)` words produced over the years for the duration of the program between `r paste0(min(theses_all$grad_year, na.rm = TRUE), "-", max(theses_all$grad_year, na.rm = TRUE))`. The first year of the program was not represented, as PDF files were only available from 2008 onward.

# Methods

Research questions:

-   What were the main topics addressed in the EMU program?

-   How did those topics evolve throughout the duration of the program between 2007-2021?

-   To what extent were the topics and evolution thereof influenced by the assignments given to the students throughout their studies leading up to their thesis?

-   To what extent were the topics and evolution thereof influenced by the exchange semester followed by the students?

## Data collection

The theses are available in PDF format with complex layouts typical to an urbanism project in which text and various types of graphics are combined.

A total number of `r nrow(emu_theses)` out of `r nrow(theses_all)` were analysed.

```{r}
#| label: fig-map-theses
#| warning: false 
#| echo: false
#| fig-cap: "Location of EMU theses included in the analysis"

# Geocode thesis locations
emu_theses <- emu_theses %>%
  geocode(location, method = 'osm', lat = latitude , long = longitude)

# Show thesis locations on world map
world <- map_data("world")
ggplot() +
  geom_map(
    data = world, map = world,
    aes(long, lat, map_id = region),
    color = "black", fill = "lightgray", size = 0.1
  ) +
  geom_point(data = emu_theses, aes(longitude, latitude), color = "red") +
  coord_fixed() +
  theme_void()
```

Figure @fig-map-theses shows the geographic spread of the theses included in the analysis.

## Data analysis

[@silge2017a] [@hvitfeldt2021a]

### Tokenisation

Word-level tokenisation was applied with the `unnest_tokens()` function of the `tidytext` package. This was preferred over other tokenisers as it produced results that work seamlessly with other `tidyverse` tools.

### Stop words

-   Three types of stop words: global, subject-specific and document stop words.
-   Global stop words:
    -   All theses were written in English, so for common English stop words the "snowball" lexicon was used
    -   Optionally, try creating a custom stop word list on the entire corpus, by removing high-frequency words, starting with 20 words and increasing by 10 until reaching words that are not appropriate as stop words
-   Subject-specific stop words:
    -   Manually constructed list, needs domain knowledge
    -   Words that are commonly used to structure a theses, such as "preface", "contents", and "introduction", were added as custom stop words
-   Document stop words:
    -   Stop words from titles and author names should only be removed from corresponding theses

```{r}
#| label: stop-words

stop_words_custom <- emu_theses %>% 
  unnest_tokens(output = word, input = title) %>%   ### add words from the title
  select(word) %>% 
  rbind(., data.frame(word = c("preface", "foreword", "introduction", "conclusion", "thesis", 
                               "source", "author"))) %>% 
  # rbind(., data.frame(word = c("city", "urban", "urbanism"))) %>% 
  rbind(., data.frame(word = c("hab", "km")))

emu_theses_words <- emu_theses %>%
  mutate(author = paste(first_name, last_name)) %>% 
  select(author, title, text, -c(graduation_year, first_name, last_name)) %>% 
  unnest_tokens(output = word, input = text) %>%   # remove punctuation, convert to lowercase, seperate all words
  anti_join(stop_words, by = "word") %>%  # remove stop words
  anti_join(stop_words_custom, by = "word")
```

Stemming is a pre-processing step that needs to be carefully thought through, as it might either degrade topic modeling or produce no meaningful results [@hvitfeldt2021a].

```{r}
#| label: stemming

emu_theses_words %>% 
  mutate(stem = wordStem(words = word, 
                         language = "porter")) %>%  # Implement the Porter stemming algorithm for English provided by the SnowballC package
  count(stem, sort = TRUE)
```

## Topic model

Similar to clustering on numeric data, topic modeling is a method of unsupervised classification of topics found in a group of documents [@silge2017].

```{r}
# The topicmodels package requires a document-term matrix as input
emu_theses_dtm <- emu_theses_words |> 
  count(title, word, sort = TRUE) |> 
  cast_dtm(title, word, n)

emu_theses_dtm

# We will use the Latent Dirichlet Allocation (LDA) algorithm for topic modeling
# In LDA, each document is a mixture of topics and each topic is a mixture of words
emu_theses_lda <- LDA(emu_theses_dtm, k = 2, control = list(seed = 2023))
emu_theses_lda

# Tidy model with beta statistic
emu_theses_topics <- tidy(emu_theses_lda, matrix = "beta")
emu_theses_topics

# The top 10 words that are the most common in each topic
emu_theses_top_terms <- emu_theses_topics |> 
  group_by(topic) |> 
  top_n(10, beta) |> 
  ungroup() |> 
  arrange(topic, -beta)

emu_theses_top_terms |> 
  mutate(term = reorder(term, beta)) |> 
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

# 
beta_spread <- emu_theses_topics |> 
  mutate(topic = paste0("topic", topic)) |> 
  spread(topic, beta) |> 
  filter(topic1 > 0.001, topic2 > .001) |> 
  mutate(log_ratio = log2(topic2/topic1))

beta_spread

beta_spread %>%
  group_by(direction = log_ratio > 0) %>%
  slice_max(abs(log_ratio), n = 10) %>% 
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(log_ratio, term)) +
  geom_col() +
  labs(x = "Log2 ratio of beta in topic 2 / topic 1", y = NULL)

emu_theses_documents <- tidy(emu_theses_lda, matrix = "gamma")
emu_theses_documents
```


# Results

```{r}
# Which are the top 20 most frequently used words in each thesis?
emu_theses_top_20 <- emu_theses_words %>%
  group_by(title) %>%
  count(word, sort = TRUE) %>%
  slice_max(n, n = 20) %>%        ### top_n is superseded; better use slice_max instead
  # filter(n > 50) %>%              ### filter words with a minimum count
  ungroup() %>% 
  mutate(word = reorder(word, n))

# Which are first 5 most frequently used words in each thesis?
emu_theses_first_5 <- emu_theses_words %>%
  count(title, word) %>% 
  group_by(title) %>% 
  arrange(desc(n)) %>% 
  slice(1:5)

# Which theses contain "fabric" in the top 20 most frequently used words?
emu_theses_top_20 %>% 
  filter(word == "fabric")

emu_theses_top_20 %>%
  slice(1:40) %>% 
  ggplot(aes(x = reorder(word, n), y = n, fill = title)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(y = "Count",
       x = "Unique words",
       title = "Count of unique words found in thesis",
       subtitle = "Stop words were removed from the list") +
  facet_wrap( ~ title, scales = "free", ncol = 2)


# # wip function ngrams
# get_ngrams <- function(df, n) {
#   if (!is.numeric(n)) {
#     stop("n is not numeric")
#   }
#   emu_theses %>%
#     mutate(author = paste(first_name, last_name)) %>% 
#     select(author, title, text, -c(year, first_name, last_name, text_start, text_end)) %>% 
#     unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
#     separate(bigram, into = c("first","second"), sep = " ", remove = FALSE) %>%
#     anti_join(stop_words, by = c("first" = "word")) %>%
#     anti_join(stop_words, by = c("second" = "word")) %>%
#     anti_join(stop_words_custom, by = c("first" = "word")) %>%
#     anti_join(stop_words_custom, by = c("second" = "word")) %>%
#     filter(str_detect(first, "[a-z]") & str_detect(second, "[a-z]")) %>% 
#     filter(first != second) %>% 
#     filter(first != "source")
# }
# emu_theses_ngrams <- get_ngrams(emu_theses, 2)

# bigrams
emu_theses_bigrams <- emu_theses %>%
  mutate(author = paste(first_name, last_name)) %>% 
  select(author, title, text, -c(graduation_year, first_name, last_name)) %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, into = c("first","second"), sep = " ", remove = FALSE) %>%
  anti_join(stop_words, by = c("first" = "word")) %>%
  anti_join(stop_words, by = c("second" = "word")) %>%
  anti_join(stop_words_custom, by = c("first" = "word")) %>%
  anti_join(stop_words_custom, by = c("second" = "word")) %>%
  filter(str_detect(first, "[a-z]") & str_detect(second, "[a-z]")) %>% 
  filter(first != second) %>% 
  filter(first != "source")

emu_theses_bigrams_top <- emu_theses_bigrams %>%
  group_by(title) %>%
  count(bigram, sort = TRUE) %>%
  slice_max(n, n = 10) %>%        ### top_n is superseded; better use slice_max instead
  # filter(n > 50)                ### filter words with a minimum count
  ungroup() %>% 
  mutate(bigram = reorder(bigram, n))

emu_theses_bigrams_top_all <- emu_theses_bigrams %>%
  count(bigram, sort = TRUE) %>%
  slice_max(n, n = 20) %>%        ### top_n is superseded; better use slice_max instead
  # filter(n > 50)                ### filter words with a minimum count
  mutate(bigram = reorder(bigram, n))

emu_theses_bigrams_top %>%
  ggplot(aes(x = bigram, y = n, fill = title)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(y = "Count",
       x = "Unique words",
       title = "Count of unique words found in thesis",
       subtitle = "Stop words were removed from the list") +
  facet_wrap( ~ title, scales = "free", ncol = 2)

# find the most frequently used bigrams
emu_thesis_bigrams_top <- emu_theses_bigrams %>%
  group_by(title) %>% 
  count(bigram, sort = TRUE) %>%
  slice_max(n, n = 10) %>%                   ### change the number of top words to show
  mutate(bigram = reorder(bigram, n)) %>% 
  print()
```

# Discussion

# Conclusion

# Acknowledgements

<!-- The following line inserts a page break  -->

\newpage

# References

<!-- The following line ensures the references appear here for the MS Word or HTML output files, rather than right at the end of the document (this will not work for PDF files):  -->

::: {#refs}
:::

\newpage

### Colophon

This report was generated on `r Sys.time()` using the following computational environment and dependencies:

```{r}
#| label: colophon
#| cache: false

# which R packages and versions?
if ("devtools" %in% installed.packages()) devtools::session_info()
```

The current Git commit details are:

```{r}
# what commit is this file at? 
if ("git2r" %in% installed.packages() & git2r::in_repository(path = ".")) git2r::repository(here::here())  
```
